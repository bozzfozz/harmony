.E.E.E.E.E.E.................................................................F.......................................... [ 17%]
........................s.............................F.........................................F....................... [ 36%]
..........................................................F....F........................................................ [ 55%]
........................................................................................................................ [ 73%]
.............................................FF............F.......F.................................................... [ 92%]
......s....FF..............................F...                                                                          [100%]
============================================================ ERRORS ============================================================
_____________________________ ERROR at teardown of test_admin_dry_run_shows_delta_no_side_effects ______________________________

self = <sqlalchemy.engine.base.Connection object at 0x7f531db0ca50>
dialect = <sqlalchemy.dialects.sqlite.pysqlite.SQLiteDialect_pysqlite object at 0x7f531dad4610>
context = <sqlalchemy.dialects.sqlite.base.SQLiteExecutionContext object at 0x7f531db0cd10>
statement = <sqlalchemy.dialects.sqlite.base.SQLiteCompiler object at 0x7f531db0cd50>, parameters = [()]

    def _exec_single_context(
        self,
        dialect: Dialect,
        context: ExecutionContext,
        statement: Union[str, Compiled],
        parameters: Optional[_AnyMultiExecuteParams],
    ) -> CursorResult[Any]:
        """continue the _execute_context() method for a single DBAPI
        cursor.execute() or cursor.executemany() call.
    
        """
        if dialect.bind_typing is BindTyping.SETINPUTSIZES:
            generic_setinputsizes = context._prepare_set_input_sizes()
    
            if generic_setinputsizes:
                try:
                    dialect.do_set_input_sizes(
                        context.cursor, generic_setinputsizes, context
                    )
                except BaseException as e:
                    self._handle_dbapi_exception(
                        e, str(statement), parameters, None, context
                    )
    
        cursor, str_statement, parameters = (
            context.cursor,
            context.statement,
            context.parameters,
        )
    
        effective_parameters: Optional[_AnyExecuteParams]
    
        if not context.executemany:
            effective_parameters = parameters[0]
        else:
            effective_parameters = parameters
    
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                str_statement, effective_parameters = fn(
                    self,
                    cursor,
                    str_statement,
                    effective_parameters,
                    context,
                    context.executemany,
                )
    
        if self._echo:
            self._log_info(str_statement)
    
            stats = context._get_cache_stats()
    
            if not self.engine.hide_parameters:
                self._log_info(
                    "[%s] %r",
                    stats,
                    sql_util._repr_params(
                        effective_parameters,
                        batches=10,
                        ismulti=context.executemany,
                    ),
                )
            else:
                self._log_info(
                    "[%s] [SQL parameters hidden due to hide_parameters=True]",
                    stats,
                )
    
        evt_handled: bool = False
        try:
            if context.execute_style is ExecuteStyle.EXECUTEMANY:
                effective_parameters = cast(
                    "_CoreMultiExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        str_statement,
                        effective_parameters,
                        context,
                    )
            elif not effective_parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, str_statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor, str_statement, context
                    )
            else:
                effective_parameters = cast(
                    "_CoreSingleExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
>                   self.dialect.do_execute(
                        cursor, str_statement, effective_parameters, context
                    )

/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1967: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.sqlite.pysqlite.SQLiteDialect_pysqlite object at 0x7f531dad4610>
cursor = <sqlite3.Cursor object at 0x7f531daf61c0>, statement = 'DELETE FROM activity_events', parameters = ()
context = <sqlalchemy.dialects.sqlite.base.SQLiteExecutionContext object at 0x7f531db0cd10>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       sqlite3.OperationalError: no such table: activity_events

/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/default.py:951: OperationalError

The above exception was the direct cause of the following exception:

    @pytest.fixture(autouse=True)
    def reset_activity_manager() -> None:
        activity_manager.clear()
        with session_scope() as session:
            session.query(ActivityEvent).delete()
        yield
        activity_manager.clear()
        with session_scope() as session:
>           session.query(ActivityEvent).delete()

tests/conftest.py:1460: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/orm/query.py:3208: in delete
    result: CursorResult[Any] = self.session.execute(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/orm/session.py:2365: in execute
    return self._execute_internal(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/orm/session.py:2251: in _execute_internal
    result: Result[Any] = compile_state_cls.orm_execute_statement(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/orm/bulk_persistence.py:2033: in orm_execute_statement
    return super().orm_execute_statement(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/orm/context.py:306: in orm_execute_statement
    result = conn.execute(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1419: in execute
    return meth(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/sql/elements.py:526: in _execute_on_connection
    return connection._execute_clauseelement(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1641: in _execute_clauseelement
    ret = self._execute_context(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1846: in _execute_context
    return self._exec_single_context(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1986: in _exec_single_context
    self._handle_dbapi_exception(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:2355: in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1967: in _exec_single_context
    self.dialect.do_execute(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.sqlite.pysqlite.SQLiteDialect_pysqlite object at 0x7f531dad4610>
cursor = <sqlite3.Cursor object at 0x7f531daf61c0>, statement = 'DELETE FROM activity_events', parameters = ()
context = <sqlalchemy.dialects.sqlite.base.SQLiteExecutionContext object at 0x7f531db0cd10>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such table: activity_events
E       [SQL: DELETE FROM activity_events]
E       (Background on this error at: https://sqlalche.me/e/20/e3q8)

/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/default.py:951: OperationalError
---------------------------------------------------- Captured stdout setup -----------------------------------------------------
2025-10-08 06:37:58,053 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:37:58,053 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
2025-10-08 06:37:58,061 INFO  [alembic.runtime.migration] Running upgrade  -> 7c9bdb5e1a3d, Create base schema
2025-10-08 06:37:58,330 INFO  [alembic.runtime.migration] Running upgrade 7c9bdb5e1a3d -> b4e3a1f6c8f6, Add persistent retry_block_until timestamp for watchlist artists.
2025-10-08 06:37:58,337 INFO  [alembic.runtime.migration] Running upgrade b4e3a1f6c8f6 -> 8f43f3f38e0b, Create queue jobs table with idempotent guards.
2025-10-08 06:37:58,342 INFO  [alembic.runtime.migration] Running upgrade 8f43f3f38e0b -> 202406141200, Add queue job payload_json and stop_reason columns with guards.
2025-10-08 06:37:58,345 INFO  [alembic.runtime.migration] Running upgrade 202406141200 -> 202409041200, Enforce queue job idempotency uniqueness.
2025-10-08 06:37:58,347 INFO  [alembic.runtime.migration] Running upgrade 202409041200 -> 202409091200, Create partial unique index for queue job idempotency.
2025-10-08 06:37:58,354 INFO  [alembic.runtime.migration] Running upgrade 202409091200 -> 202409201200, Ensure queue job idempotency keys are globally unique.
2025-10-08 06:37:58,362 INFO  [alembic.runtime.migration] Running upgrade 202409201200 -> 202410011200, Create persistence tables for artist metadata and watchlist entries.
2025-10-08 06:37:58,365 INFO  [alembic.runtime.migration] Running upgrade 202410011200 -> 202410211200, Add last_synced_at to artist_watchlist entries.
2025-10-08 06:37:58,367 INFO  [alembic.runtime.migration] Running upgrade 202410211200 -> 202411011200, Add artist release pruning columns and audit table.
2025-10-08 06:37:58,370 INFO  [alembic.runtime.migration] Running upgrade 202411011200 -> 202411151200, Extend watchlist_artists with async scheduling columns.
2025-10-08 06:37:58,412 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:37:58,412 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
2025-10-08 06:37:58,417 INFO  [alembic.runtime.migration] Running upgrade  -> 7c9bdb5e1a3d, Create base schema
2025-10-08 06:37:58,622 INFO  [alembic.runtime.migration] Running upgrade 7c9bdb5e1a3d -> b4e3a1f6c8f6, Add persistent retry_block_until timestamp for watchlist artists.
2025-10-08 06:37:58,630 INFO  [alembic.runtime.migration] Running upgrade b4e3a1f6c8f6 -> 8f43f3f38e0b, Create queue jobs table with idempotent guards.
2025-10-08 06:37:58,645 INFO  [alembic.runtime.migration] Running upgrade 8f43f3f38e0b -> 202406141200, Add queue job payload_json and stop_reason columns with guards.
2025-10-08 06:37:58,647 INFO  [alembic.runtime.migration] Running upgrade 202406141200 -> 202409041200, Enforce queue job idempotency uniqueness.
2025-10-08 06:37:58,650 INFO  [alembic.runtime.migration] Running upgrade 202409041200 -> 202409091200, Create partial unique index for queue job idempotency.
2025-10-08 06:37:58,658 INFO  [alembic.runtime.migration] Running upgrade 202409091200 -> 202409201200, Ensure queue job idempotency keys are globally unique.
2025-10-08 06:37:58,676 INFO  [alembic.runtime.migration] Running upgrade 202409201200 -> 202410011200, Create persistence tables for artist metadata and watchlist entries.
2025-10-08 06:37:58,694 INFO  [alembic.runtime.migration] Running upgrade 202410011200 -> 202410211200, Add last_synced_at to artist_watchlist entries.
2025-10-08 06:37:58,699 INFO  [alembic.runtime.migration] Running upgrade 202410211200 -> 202411011200, Add artist release pruning columns and audit table.
2025-10-08 06:37:58,702 INFO  [alembic.runtime.migration] Running upgrade 202411011200 -> 202411151200, Extend watchlist_artists with async scheduling columns.
----------------------------------------------------- Captured stdout call -----------------------------------------------------
2025-10-08 06:37:58,717 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:37:58,717 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
2025-10-08 06:37:58,759 INFO  [app.middleware.logging] api.request
______________________________ ERROR at teardown of test_admin_apply_reconcile_updates_and_audits ______________________________

self = <sqlalchemy.engine.base.Connection object at 0x7f531dc41fd0>
dialect = <sqlalchemy.dialects.sqlite.pysqlite.SQLiteDialect_pysqlite object at 0x7f531dad7c90>
context = <sqlalchemy.dialects.sqlite.base.SQLiteExecutionContext object at 0x7f531dc42b90>
statement = <sqlalchemy.dialects.sqlite.base.SQLiteCompiler object at 0x7f531dc41f50>, parameters = [()]

    def _exec_single_context(
        self,
        dialect: Dialect,
        context: ExecutionContext,
        statement: Union[str, Compiled],
        parameters: Optional[_AnyMultiExecuteParams],
    ) -> CursorResult[Any]:
        """continue the _execute_context() method for a single DBAPI
        cursor.execute() or cursor.executemany() call.
    
        """
        if dialect.bind_typing is BindTyping.SETINPUTSIZES:
            generic_setinputsizes = context._prepare_set_input_sizes()
    
            if generic_setinputsizes:
                try:
                    dialect.do_set_input_sizes(
                        context.cursor, generic_setinputsizes, context
                    )
                except BaseException as e:
                    self._handle_dbapi_exception(
                        e, str(statement), parameters, None, context
                    )
    
        cursor, str_statement, parameters = (
            context.cursor,
            context.statement,
            context.parameters,
        )
    
        effective_parameters: Optional[_AnyExecuteParams]
    
        if not context.executemany:
            effective_parameters = parameters[0]
        else:
            effective_parameters = parameters
    
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                str_statement, effective_parameters = fn(
                    self,
                    cursor,
                    str_statement,
                    effective_parameters,
                    context,
                    context.executemany,
                )
    
        if self._echo:
            self._log_info(str_statement)
    
            stats = context._get_cache_stats()
    
            if not self.engine.hide_parameters:
                self._log_info(
                    "[%s] %r",
                    stats,
                    sql_util._repr_params(
                        effective_parameters,
                        batches=10,
                        ismulti=context.executemany,
                    ),
                )
            else:
                self._log_info(
                    "[%s] [SQL parameters hidden due to hide_parameters=True]",
                    stats,
                )
    
        evt_handled: bool = False
        try:
            if context.execute_style is ExecuteStyle.EXECUTEMANY:
                effective_parameters = cast(
                    "_CoreMultiExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        str_statement,
                        effective_parameters,
                        context,
                    )
            elif not effective_parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, str_statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor, str_statement, context
                    )
            else:
                effective_parameters = cast(
                    "_CoreSingleExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
>                   self.dialect.do_execute(
                        cursor, str_statement, effective_parameters, context
                    )

/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1967: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.sqlite.pysqlite.SQLiteDialect_pysqlite object at 0x7f531dad7c90>
cursor = <sqlite3.Cursor object at 0x7f5317d106c0>, statement = 'DELETE FROM activity_events', parameters = ()
context = <sqlalchemy.dialects.sqlite.base.SQLiteExecutionContext object at 0x7f531dc42b90>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       sqlite3.OperationalError: no such table: activity_events

/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/default.py:951: OperationalError

The above exception was the direct cause of the following exception:

    @pytest.fixture(autouse=True)
    def reset_activity_manager() -> None:
        activity_manager.clear()
        with session_scope() as session:
            session.query(ActivityEvent).delete()
        yield
        activity_manager.clear()
        with session_scope() as session:
>           session.query(ActivityEvent).delete()

tests/conftest.py:1460: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/orm/query.py:3208: in delete
    result: CursorResult[Any] = self.session.execute(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/orm/session.py:2365: in execute
    return self._execute_internal(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/orm/session.py:2251: in _execute_internal
    result: Result[Any] = compile_state_cls.orm_execute_statement(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/orm/bulk_persistence.py:2033: in orm_execute_statement
    return super().orm_execute_statement(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/orm/context.py:306: in orm_execute_statement
    result = conn.execute(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1419: in execute
    return meth(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/sql/elements.py:526: in _execute_on_connection
    return connection._execute_clauseelement(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1641: in _execute_clauseelement
    ret = self._execute_context(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1846: in _execute_context
    return self._exec_single_context(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1986: in _exec_single_context
    self._handle_dbapi_exception(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:2355: in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1967: in _exec_single_context
    self.dialect.do_execute(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.sqlite.pysqlite.SQLiteDialect_pysqlite object at 0x7f531dad7c90>
cursor = <sqlite3.Cursor object at 0x7f5317d106c0>, statement = 'DELETE FROM activity_events', parameters = ()
context = <sqlalchemy.dialects.sqlite.base.SQLiteExecutionContext object at 0x7f531dc42b90>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such table: activity_events
E       [SQL: DELETE FROM activity_events]
E       (Background on this error at: https://sqlalche.me/e/20/e3q8)

/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/default.py:951: OperationalError
---------------------------------------------------- Captured stdout setup -----------------------------------------------------
2025-10-08 06:37:59,424 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:37:59,424 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
2025-10-08 06:37:59,431 INFO  [alembic.runtime.migration] Running upgrade  -> 7c9bdb5e1a3d, Create base schema
2025-10-08 06:37:59,662 INFO  [alembic.runtime.migration] Running upgrade 7c9bdb5e1a3d -> b4e3a1f6c8f6, Add persistent retry_block_until timestamp for watchlist artists.
2025-10-08 06:37:59,667 INFO  [alembic.runtime.migration] Running upgrade b4e3a1f6c8f6 -> 8f43f3f38e0b, Create queue jobs table with idempotent guards.
2025-10-08 06:37:59,671 INFO  [alembic.runtime.migration] Running upgrade 8f43f3f38e0b -> 202406141200, Add queue job payload_json and stop_reason columns with guards.
2025-10-08 06:37:59,674 INFO  [alembic.runtime.migration] Running upgrade 202406141200 -> 202409041200, Enforce queue job idempotency uniqueness.
2025-10-08 06:37:59,676 INFO  [alembic.runtime.migration] Running upgrade 202409041200 -> 202409091200, Create partial unique index for queue job idempotency.
2025-10-08 06:37:59,682 INFO  [alembic.runtime.migration] Running upgrade 202409091200 -> 202409201200, Ensure queue job idempotency keys are globally unique.
2025-10-08 06:37:59,689 INFO  [alembic.runtime.migration] Running upgrade 202409201200 -> 202410011200, Create persistence tables for artist metadata and watchlist entries.
2025-10-08 06:37:59,693 INFO  [alembic.runtime.migration] Running upgrade 202410011200 -> 202410211200, Add last_synced_at to artist_watchlist entries.
2025-10-08 06:37:59,695 INFO  [alembic.runtime.migration] Running upgrade 202410211200 -> 202411011200, Add artist release pruning columns and audit table.
2025-10-08 06:37:59,697 INFO  [alembic.runtime.migration] Running upgrade 202411011200 -> 202411151200, Extend watchlist_artists with async scheduling columns.
2025-10-08 06:37:59,721 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:37:59,721 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
2025-10-08 06:37:59,726 INFO  [alembic.runtime.migration] Running upgrade  -> 7c9bdb5e1a3d, Create base schema
2025-10-08 06:37:59,928 INFO  [alembic.runtime.migration] Running upgrade 7c9bdb5e1a3d -> b4e3a1f6c8f6, Add persistent retry_block_until timestamp for watchlist artists.
2025-10-08 06:37:59,934 INFO  [alembic.runtime.migration] Running upgrade b4e3a1f6c8f6 -> 8f43f3f38e0b, Create queue jobs table with idempotent guards.
2025-10-08 06:37:59,938 INFO  [alembic.runtime.migration] Running upgrade 8f43f3f38e0b -> 202406141200, Add queue job payload_json and stop_reason columns with guards.
2025-10-08 06:37:59,952 INFO  [alembic.runtime.migration] Running upgrade 202406141200 -> 202409041200, Enforce queue job idempotency uniqueness.
2025-10-08 06:37:59,955 INFO  [alembic.runtime.migration] Running upgrade 202409041200 -> 202409091200, Create partial unique index for queue job idempotency.
2025-10-08 06:37:59,961 INFO  [alembic.runtime.migration] Running upgrade 202409091200 -> 202409201200, Ensure queue job idempotency keys are globally unique.
2025-10-08 06:37:59,968 INFO  [alembic.runtime.migration] Running upgrade 202409201200 -> 202410011200, Create persistence tables for artist metadata and watchlist entries.
2025-10-08 06:37:59,971 INFO  [alembic.runtime.migration] Running upgrade 202410011200 -> 202410211200, Add last_synced_at to artist_watchlist entries.
2025-10-08 06:37:59,973 INFO  [alembic.runtime.migration] Running upgrade 202410211200 -> 202411011200, Add artist release pruning columns and audit table.
2025-10-08 06:37:59,976 INFO  [alembic.runtime.migration] Running upgrade 202411011200 -> 202411151200, Extend watchlist_artists with async scheduling columns.
----------------------------------------------------- Captured stdout call -----------------------------------------------------
2025-10-08 06:37:59,986 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:37:59,986 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
_________________________ ERROR at teardown of test_admin_resync_enqueues_with_priority_and_lock_guard _________________________

self = <sqlalchemy.engine.base.Connection object at 0x7f531dc40a50>
dialect = <sqlalchemy.dialects.sqlite.pysqlite.SQLiteDialect_pysqlite object at 0x7f531dc476d0>
context = <sqlalchemy.dialects.sqlite.base.SQLiteExecutionContext object at 0x7f531dc43050>
statement = <sqlalchemy.dialects.sqlite.base.SQLiteCompiler object at 0x7f531dc42cd0>, parameters = [()]

    def _exec_single_context(
        self,
        dialect: Dialect,
        context: ExecutionContext,
        statement: Union[str, Compiled],
        parameters: Optional[_AnyMultiExecuteParams],
    ) -> CursorResult[Any]:
        """continue the _execute_context() method for a single DBAPI
        cursor.execute() or cursor.executemany() call.
    
        """
        if dialect.bind_typing is BindTyping.SETINPUTSIZES:
            generic_setinputsizes = context._prepare_set_input_sizes()
    
            if generic_setinputsizes:
                try:
                    dialect.do_set_input_sizes(
                        context.cursor, generic_setinputsizes, context
                    )
                except BaseException as e:
                    self._handle_dbapi_exception(
                        e, str(statement), parameters, None, context
                    )
    
        cursor, str_statement, parameters = (
            context.cursor,
            context.statement,
            context.parameters,
        )
    
        effective_parameters: Optional[_AnyExecuteParams]
    
        if not context.executemany:
            effective_parameters = parameters[0]
        else:
            effective_parameters = parameters
    
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                str_statement, effective_parameters = fn(
                    self,
                    cursor,
                    str_statement,
                    effective_parameters,
                    context,
                    context.executemany,
                )
    
        if self._echo:
            self._log_info(str_statement)
    
            stats = context._get_cache_stats()
    
            if not self.engine.hide_parameters:
                self._log_info(
                    "[%s] %r",
                    stats,
                    sql_util._repr_params(
                        effective_parameters,
                        batches=10,
                        ismulti=context.executemany,
                    ),
                )
            else:
                self._log_info(
                    "[%s] [SQL parameters hidden due to hide_parameters=True]",
                    stats,
                )
    
        evt_handled: bool = False
        try:
            if context.execute_style is ExecuteStyle.EXECUTEMANY:
                effective_parameters = cast(
                    "_CoreMultiExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        str_statement,
                        effective_parameters,
                        context,
                    )
            elif not effective_parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, str_statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor, str_statement, context
                    )
            else:
                effective_parameters = cast(
                    "_CoreSingleExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
>                   self.dialect.do_execute(
                        cursor, str_statement, effective_parameters, context
                    )

/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1967: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.sqlite.pysqlite.SQLiteDialect_pysqlite object at 0x7f531dc476d0>
cursor = <sqlite3.Cursor object at 0x7f531dae4740>, statement = 'DELETE FROM activity_events', parameters = ()
context = <sqlalchemy.dialects.sqlite.base.SQLiteExecutionContext object at 0x7f531dc43050>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       sqlite3.OperationalError: no such table: activity_events

/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/default.py:951: OperationalError

The above exception was the direct cause of the following exception:

    @pytest.fixture(autouse=True)
    def reset_activity_manager() -> None:
        activity_manager.clear()
        with session_scope() as session:
            session.query(ActivityEvent).delete()
        yield
        activity_manager.clear()
        with session_scope() as session:
>           session.query(ActivityEvent).delete()

tests/conftest.py:1460: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/orm/query.py:3208: in delete
    result: CursorResult[Any] = self.session.execute(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/orm/session.py:2365: in execute
    return self._execute_internal(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/orm/session.py:2251: in _execute_internal
    result: Result[Any] = compile_state_cls.orm_execute_statement(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/orm/bulk_persistence.py:2033: in orm_execute_statement
    return super().orm_execute_statement(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/orm/context.py:306: in orm_execute_statement
    result = conn.execute(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1419: in execute
    return meth(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/sql/elements.py:526: in _execute_on_connection
    return connection._execute_clauseelement(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1641: in _execute_clauseelement
    ret = self._execute_context(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1846: in _execute_context
    return self._exec_single_context(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1986: in _exec_single_context
    self._handle_dbapi_exception(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:2355: in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1967: in _exec_single_context
    self.dialect.do_execute(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.sqlite.pysqlite.SQLiteDialect_pysqlite object at 0x7f531dc476d0>
cursor = <sqlite3.Cursor object at 0x7f531dae4740>, statement = 'DELETE FROM activity_events', parameters = ()
context = <sqlalchemy.dialects.sqlite.base.SQLiteExecutionContext object at 0x7f531dc43050>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such table: activity_events
E       [SQL: DELETE FROM activity_events]
E       (Background on this error at: https://sqlalche.me/e/20/e3q8)

/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/default.py:951: OperationalError
---------------------------------------------------- Captured stdout setup -----------------------------------------------------
2025-10-08 06:38:00,488 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:38:00,488 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
2025-10-08 06:38:00,494 INFO  [alembic.runtime.migration] Running upgrade  -> 7c9bdb5e1a3d, Create base schema
2025-10-08 06:38:00,712 INFO  [alembic.runtime.migration] Running upgrade 7c9bdb5e1a3d -> b4e3a1f6c8f6, Add persistent retry_block_until timestamp for watchlist artists.
2025-10-08 06:38:00,717 INFO  [alembic.runtime.migration] Running upgrade b4e3a1f6c8f6 -> 8f43f3f38e0b, Create queue jobs table with idempotent guards.
2025-10-08 06:38:00,723 INFO  [alembic.runtime.migration] Running upgrade 8f43f3f38e0b -> 202406141200, Add queue job payload_json and stop_reason columns with guards.
2025-10-08 06:38:00,726 INFO  [alembic.runtime.migration] Running upgrade 202406141200 -> 202409041200, Enforce queue job idempotency uniqueness.
2025-10-08 06:38:00,728 INFO  [alembic.runtime.migration] Running upgrade 202409041200 -> 202409091200, Create partial unique index for queue job idempotency.
2025-10-08 06:38:00,734 INFO  [alembic.runtime.migration] Running upgrade 202409091200 -> 202409201200, Ensure queue job idempotency keys are globally unique.
2025-10-08 06:38:00,742 INFO  [alembic.runtime.migration] Running upgrade 202409201200 -> 202410011200, Create persistence tables for artist metadata and watchlist entries.
2025-10-08 06:38:00,745 INFO  [alembic.runtime.migration] Running upgrade 202410011200 -> 202410211200, Add last_synced_at to artist_watchlist entries.
2025-10-08 06:38:00,747 INFO  [alembic.runtime.migration] Running upgrade 202410211200 -> 202411011200, Add artist release pruning columns and audit table.
2025-10-08 06:38:00,750 INFO  [alembic.runtime.migration] Running upgrade 202411011200 -> 202411151200, Extend watchlist_artists with async scheduling columns.
2025-10-08 06:38:00,774 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:38:00,774 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
2025-10-08 06:38:00,779 INFO  [alembic.runtime.migration] Running upgrade  -> 7c9bdb5e1a3d, Create base schema
2025-10-08 06:38:00,999 INFO  [alembic.runtime.migration] Running upgrade 7c9bdb5e1a3d -> b4e3a1f6c8f6, Add persistent retry_block_until timestamp for watchlist artists.
2025-10-08 06:38:01,004 INFO  [alembic.runtime.migration] Running upgrade b4e3a1f6c8f6 -> 8f43f3f38e0b, Create queue jobs table with idempotent guards.
2025-10-08 06:38:01,009 INFO  [alembic.runtime.migration] Running upgrade 8f43f3f38e0b -> 202406141200, Add queue job payload_json and stop_reason columns with guards.
2025-10-08 06:38:01,012 INFO  [alembic.runtime.migration] Running upgrade 202406141200 -> 202409041200, Enforce queue job idempotency uniqueness.
2025-10-08 06:38:01,014 INFO  [alembic.runtime.migration] Running upgrade 202409041200 -> 202409091200, Create partial unique index for queue job idempotency.
2025-10-08 06:38:01,021 INFO  [alembic.runtime.migration] Running upgrade 202409091200 -> 202409201200, Ensure queue job idempotency keys are globally unique.
2025-10-08 06:38:01,029 INFO  [alembic.runtime.migration] Running upgrade 202409201200 -> 202410011200, Create persistence tables for artist metadata and watchlist entries.
2025-10-08 06:38:01,032 INFO  [alembic.runtime.migration] Running upgrade 202410011200 -> 202410211200, Add last_synced_at to artist_watchlist entries.
2025-10-08 06:38:01,034 INFO  [alembic.runtime.migration] Running upgrade 202410211200 -> 202411011200, Add artist release pruning columns and audit table.
2025-10-08 06:38:01,037 INFO  [alembic.runtime.migration] Running upgrade 202411011200 -> 202411151200, Extend watchlist_artists with async scheduling columns.
----------------------------------------------------- Captured stdout call -----------------------------------------------------
2025-10-08 06:38:01,047 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:38:01,047 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
2025-10-08 06:38:01,162 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:38:01,162 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
_____________________________ ERROR at teardown of test_admin_audit_lists_recent_events_paginated ______________________________

self = <sqlalchemy.engine.base.Connection object at 0x7f5317cdda90>
dialect = <sqlalchemy.dialects.sqlite.pysqlite.SQLiteDialect_pysqlite object at 0x7f5317a7ea10>
context = <sqlalchemy.dialects.sqlite.base.SQLiteExecutionContext object at 0x7f531dd59f50>
statement = <sqlalchemy.dialects.sqlite.base.SQLiteCompiler object at 0x7f531dd5af50>, parameters = [()]

    def _exec_single_context(
        self,
        dialect: Dialect,
        context: ExecutionContext,
        statement: Union[str, Compiled],
        parameters: Optional[_AnyMultiExecuteParams],
    ) -> CursorResult[Any]:
        """continue the _execute_context() method for a single DBAPI
        cursor.execute() or cursor.executemany() call.
    
        """
        if dialect.bind_typing is BindTyping.SETINPUTSIZES:
            generic_setinputsizes = context._prepare_set_input_sizes()
    
            if generic_setinputsizes:
                try:
                    dialect.do_set_input_sizes(
                        context.cursor, generic_setinputsizes, context
                    )
                except BaseException as e:
                    self._handle_dbapi_exception(
                        e, str(statement), parameters, None, context
                    )
    
        cursor, str_statement, parameters = (
            context.cursor,
            context.statement,
            context.parameters,
        )
    
        effective_parameters: Optional[_AnyExecuteParams]
    
        if not context.executemany:
            effective_parameters = parameters[0]
        else:
            effective_parameters = parameters
    
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                str_statement, effective_parameters = fn(
                    self,
                    cursor,
                    str_statement,
                    effective_parameters,
                    context,
                    context.executemany,
                )
    
        if self._echo:
            self._log_info(str_statement)
    
            stats = context._get_cache_stats()
    
            if not self.engine.hide_parameters:
                self._log_info(
                    "[%s] %r",
                    stats,
                    sql_util._repr_params(
                        effective_parameters,
                        batches=10,
                        ismulti=context.executemany,
                    ),
                )
            else:
                self._log_info(
                    "[%s] [SQL parameters hidden due to hide_parameters=True]",
                    stats,
                )
    
        evt_handled: bool = False
        try:
            if context.execute_style is ExecuteStyle.EXECUTEMANY:
                effective_parameters = cast(
                    "_CoreMultiExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        str_statement,
                        effective_parameters,
                        context,
                    )
            elif not effective_parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, str_statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor, str_statement, context
                    )
            else:
                effective_parameters = cast(
                    "_CoreSingleExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
>                   self.dialect.do_execute(
                        cursor, str_statement, effective_parameters, context
                    )

/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1967: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.sqlite.pysqlite.SQLiteDialect_pysqlite object at 0x7f5317a7ea10>
cursor = <sqlite3.Cursor object at 0x7f5317f06c40>, statement = 'DELETE FROM activity_events', parameters = ()
context = <sqlalchemy.dialects.sqlite.base.SQLiteExecutionContext object at 0x7f531dd59f50>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       sqlite3.OperationalError: no such table: activity_events

/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/default.py:951: OperationalError

The above exception was the direct cause of the following exception:

    @pytest.fixture(autouse=True)
    def reset_activity_manager() -> None:
        activity_manager.clear()
        with session_scope() as session:
            session.query(ActivityEvent).delete()
        yield
        activity_manager.clear()
        with session_scope() as session:
>           session.query(ActivityEvent).delete()

tests/conftest.py:1460: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/orm/query.py:3208: in delete
    result: CursorResult[Any] = self.session.execute(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/orm/session.py:2365: in execute
    return self._execute_internal(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/orm/session.py:2251: in _execute_internal
    result: Result[Any] = compile_state_cls.orm_execute_statement(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/orm/bulk_persistence.py:2033: in orm_execute_statement
    return super().orm_execute_statement(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/orm/context.py:306: in orm_execute_statement
    result = conn.execute(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1419: in execute
    return meth(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/sql/elements.py:526: in _execute_on_connection
    return connection._execute_clauseelement(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1641: in _execute_clauseelement
    ret = self._execute_context(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1846: in _execute_context
    return self._exec_single_context(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1986: in _exec_single_context
    self._handle_dbapi_exception(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:2355: in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1967: in _exec_single_context
    self.dialect.do_execute(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.sqlite.pysqlite.SQLiteDialect_pysqlite object at 0x7f5317a7ea10>
cursor = <sqlite3.Cursor object at 0x7f5317f06c40>, statement = 'DELETE FROM activity_events', parameters = ()
context = <sqlalchemy.dialects.sqlite.base.SQLiteExecutionContext object at 0x7f531dd59f50>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such table: activity_events
E       [SQL: DELETE FROM activity_events]
E       (Background on this error at: https://sqlalche.me/e/20/e3q8)

/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/default.py:951: OperationalError
---------------------------------------------------- Captured stdout setup -----------------------------------------------------
2025-10-08 06:38:01,601 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:38:01,601 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
2025-10-08 06:38:01,608 INFO  [alembic.runtime.migration] Running upgrade  -> 7c9bdb5e1a3d, Create base schema
2025-10-08 06:38:01,853 INFO  [alembic.runtime.migration] Running upgrade 7c9bdb5e1a3d -> b4e3a1f6c8f6, Add persistent retry_block_until timestamp for watchlist artists.
2025-10-08 06:38:01,858 INFO  [alembic.runtime.migration] Running upgrade b4e3a1f6c8f6 -> 8f43f3f38e0b, Create queue jobs table with idempotent guards.
2025-10-08 06:38:01,863 INFO  [alembic.runtime.migration] Running upgrade 8f43f3f38e0b -> 202406141200, Add queue job payload_json and stop_reason columns with guards.
2025-10-08 06:38:01,866 INFO  [alembic.runtime.migration] Running upgrade 202406141200 -> 202409041200, Enforce queue job idempotency uniqueness.
2025-10-08 06:38:01,868 INFO  [alembic.runtime.migration] Running upgrade 202409041200 -> 202409091200, Create partial unique index for queue job idempotency.
2025-10-08 06:38:01,874 INFO  [alembic.runtime.migration] Running upgrade 202409091200 -> 202409201200, Ensure queue job idempotency keys are globally unique.
2025-10-08 06:38:01,882 INFO  [alembic.runtime.migration] Running upgrade 202409201200 -> 202410011200, Create persistence tables for artist metadata and watchlist entries.
2025-10-08 06:38:01,885 INFO  [alembic.runtime.migration] Running upgrade 202410011200 -> 202410211200, Add last_synced_at to artist_watchlist entries.
2025-10-08 06:38:01,887 INFO  [alembic.runtime.migration] Running upgrade 202410211200 -> 202411011200, Add artist release pruning columns and audit table.
2025-10-08 06:38:01,890 INFO  [alembic.runtime.migration] Running upgrade 202411011200 -> 202411151200, Extend watchlist_artists with async scheduling columns.
2025-10-08 06:38:01,912 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:38:01,912 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
2025-10-08 06:38:01,918 INFO  [alembic.runtime.migration] Running upgrade  -> 7c9bdb5e1a3d, Create base schema
2025-10-08 06:38:02,168 INFO  [alembic.runtime.migration] Running upgrade 7c9bdb5e1a3d -> b4e3a1f6c8f6, Add persistent retry_block_until timestamp for watchlist artists.
2025-10-08 06:38:02,174 INFO  [alembic.runtime.migration] Running upgrade b4e3a1f6c8f6 -> 8f43f3f38e0b, Create queue jobs table with idempotent guards.
2025-10-08 06:38:02,179 INFO  [alembic.runtime.migration] Running upgrade 8f43f3f38e0b -> 202406141200, Add queue job payload_json and stop_reason columns with guards.
2025-10-08 06:38:02,182 INFO  [alembic.runtime.migration] Running upgrade 202406141200 -> 202409041200, Enforce queue job idempotency uniqueness.
2025-10-08 06:38:02,184 INFO  [alembic.runtime.migration] Running upgrade 202409041200 -> 202409091200, Create partial unique index for queue job idempotency.
2025-10-08 06:38:02,191 INFO  [alembic.runtime.migration] Running upgrade 202409091200 -> 202409201200, Ensure queue job idempotency keys are globally unique.
2025-10-08 06:38:02,199 INFO  [alembic.runtime.migration] Running upgrade 202409201200 -> 202410011200, Create persistence tables for artist metadata and watchlist entries.
2025-10-08 06:38:02,202 INFO  [alembic.runtime.migration] Running upgrade 202410011200 -> 202410211200, Add last_synced_at to artist_watchlist entries.
2025-10-08 06:38:02,204 INFO  [alembic.runtime.migration] Running upgrade 202410211200 -> 202411011200, Add artist release pruning columns and audit table.
2025-10-08 06:38:02,207 INFO  [alembic.runtime.migration] Running upgrade 202411011200 -> 202411151200, Extend watchlist_artists with async scheduling columns.
----------------------------------------------------- Captured stdout call -----------------------------------------------------
2025-10-08 06:38:02,217 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:38:02,218 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
_________________________________ ERROR at teardown of test_admin_invalidate_busts_cache_etag __________________________________

self = <sqlalchemy.engine.base.Connection object at 0x7f5317a7f5d0>
dialect = <sqlalchemy.dialects.sqlite.pysqlite.SQLiteDialect_pysqlite object at 0x7f5317cdd5d0>
context = <sqlalchemy.dialects.sqlite.base.SQLiteExecutionContext object at 0x7f5317a7c390>
statement = <sqlalchemy.dialects.sqlite.base.SQLiteCompiler object at 0x7f5317a7c950>, parameters = [()]

    def _exec_single_context(
        self,
        dialect: Dialect,
        context: ExecutionContext,
        statement: Union[str, Compiled],
        parameters: Optional[_AnyMultiExecuteParams],
    ) -> CursorResult[Any]:
        """continue the _execute_context() method for a single DBAPI
        cursor.execute() or cursor.executemany() call.
    
        """
        if dialect.bind_typing is BindTyping.SETINPUTSIZES:
            generic_setinputsizes = context._prepare_set_input_sizes()
    
            if generic_setinputsizes:
                try:
                    dialect.do_set_input_sizes(
                        context.cursor, generic_setinputsizes, context
                    )
                except BaseException as e:
                    self._handle_dbapi_exception(
                        e, str(statement), parameters, None, context
                    )
    
        cursor, str_statement, parameters = (
            context.cursor,
            context.statement,
            context.parameters,
        )
    
        effective_parameters: Optional[_AnyExecuteParams]
    
        if not context.executemany:
            effective_parameters = parameters[0]
        else:
            effective_parameters = parameters
    
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                str_statement, effective_parameters = fn(
                    self,
                    cursor,
                    str_statement,
                    effective_parameters,
                    context,
                    context.executemany,
                )
    
        if self._echo:
            self._log_info(str_statement)
    
            stats = context._get_cache_stats()
    
            if not self.engine.hide_parameters:
                self._log_info(
                    "[%s] %r",
                    stats,
                    sql_util._repr_params(
                        effective_parameters,
                        batches=10,
                        ismulti=context.executemany,
                    ),
                )
            else:
                self._log_info(
                    "[%s] [SQL parameters hidden due to hide_parameters=True]",
                    stats,
                )
    
        evt_handled: bool = False
        try:
            if context.execute_style is ExecuteStyle.EXECUTEMANY:
                effective_parameters = cast(
                    "_CoreMultiExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        str_statement,
                        effective_parameters,
                        context,
                    )
            elif not effective_parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, str_statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor, str_statement, context
                    )
            else:
                effective_parameters = cast(
                    "_CoreSingleExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
>                   self.dialect.do_execute(
                        cursor, str_statement, effective_parameters, context
                    )

/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1967: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.sqlite.pysqlite.SQLiteDialect_pysqlite object at 0x7f5317cdd5d0>
cursor = <sqlite3.Cursor object at 0x7f5317d110c0>, statement = 'DELETE FROM activity_events', parameters = ()
context = <sqlalchemy.dialects.sqlite.base.SQLiteExecutionContext object at 0x7f5317a7c390>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       sqlite3.OperationalError: no such table: activity_events

/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/default.py:951: OperationalError

The above exception was the direct cause of the following exception:

    @pytest.fixture(autouse=True)
    def reset_activity_manager() -> None:
        activity_manager.clear()
        with session_scope() as session:
            session.query(ActivityEvent).delete()
        yield
        activity_manager.clear()
        with session_scope() as session:
>           session.query(ActivityEvent).delete()

tests/conftest.py:1460: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/orm/query.py:3208: in delete
    result: CursorResult[Any] = self.session.execute(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/orm/session.py:2365: in execute
    return self._execute_internal(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/orm/session.py:2251: in _execute_internal
    result: Result[Any] = compile_state_cls.orm_execute_statement(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/orm/bulk_persistence.py:2033: in orm_execute_statement
    return super().orm_execute_statement(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/orm/context.py:306: in orm_execute_statement
    result = conn.execute(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1419: in execute
    return meth(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/sql/elements.py:526: in _execute_on_connection
    return connection._execute_clauseelement(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1641: in _execute_clauseelement
    ret = self._execute_context(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1846: in _execute_context
    return self._exec_single_context(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1986: in _exec_single_context
    self._handle_dbapi_exception(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:2355: in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1967: in _exec_single_context
    self.dialect.do_execute(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.sqlite.pysqlite.SQLiteDialect_pysqlite object at 0x7f5317cdd5d0>
cursor = <sqlite3.Cursor object at 0x7f5317d110c0>, statement = 'DELETE FROM activity_events', parameters = ()
context = <sqlalchemy.dialects.sqlite.base.SQLiteExecutionContext object at 0x7f5317a7c390>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such table: activity_events
E       [SQL: DELETE FROM activity_events]
E       (Background on this error at: https://sqlalche.me/e/20/e3q8)

/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/default.py:951: OperationalError
---------------------------------------------------- Captured stdout setup -----------------------------------------------------
2025-10-08 06:38:02,820 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:38:02,820 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
2025-10-08 06:38:02,826 INFO  [alembic.runtime.migration] Running upgrade  -> 7c9bdb5e1a3d, Create base schema
2025-10-08 06:38:03,081 INFO  [alembic.runtime.migration] Running upgrade 7c9bdb5e1a3d -> b4e3a1f6c8f6, Add persistent retry_block_until timestamp for watchlist artists.
2025-10-08 06:38:03,087 INFO  [alembic.runtime.migration] Running upgrade b4e3a1f6c8f6 -> 8f43f3f38e0b, Create queue jobs table with idempotent guards.
2025-10-08 06:38:03,093 INFO  [alembic.runtime.migration] Running upgrade 8f43f3f38e0b -> 202406141200, Add queue job payload_json and stop_reason columns with guards.
2025-10-08 06:38:03,096 INFO  [alembic.runtime.migration] Running upgrade 202406141200 -> 202409041200, Enforce queue job idempotency uniqueness.
2025-10-08 06:38:03,099 INFO  [alembic.runtime.migration] Running upgrade 202409041200 -> 202409091200, Create partial unique index for queue job idempotency.
2025-10-08 06:38:03,106 INFO  [alembic.runtime.migration] Running upgrade 202409091200 -> 202409201200, Ensure queue job idempotency keys are globally unique.
2025-10-08 06:38:03,116 INFO  [alembic.runtime.migration] Running upgrade 202409201200 -> 202410011200, Create persistence tables for artist metadata and watchlist entries.
2025-10-08 06:38:03,120 INFO  [alembic.runtime.migration] Running upgrade 202410011200 -> 202410211200, Add last_synced_at to artist_watchlist entries.
2025-10-08 06:38:03,123 INFO  [alembic.runtime.migration] Running upgrade 202410211200 -> 202411011200, Add artist release pruning columns and audit table.
2025-10-08 06:38:03,126 INFO  [alembic.runtime.migration] Running upgrade 202411011200 -> 202411151200, Extend watchlist_artists with async scheduling columns.
2025-10-08 06:38:03,157 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:38:03,157 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
2025-10-08 06:38:03,164 INFO  [alembic.runtime.migration] Running upgrade  -> 7c9bdb5e1a3d, Create base schema
2025-10-08 06:38:03,460 INFO  [alembic.runtime.migration] Running upgrade 7c9bdb5e1a3d -> b4e3a1f6c8f6, Add persistent retry_block_until timestamp for watchlist artists.
2025-10-08 06:38:03,467 INFO  [alembic.runtime.migration] Running upgrade b4e3a1f6c8f6 -> 8f43f3f38e0b, Create queue jobs table with idempotent guards.
2025-10-08 06:38:03,475 INFO  [alembic.runtime.migration] Running upgrade 8f43f3f38e0b -> 202406141200, Add queue job payload_json and stop_reason columns with guards.
2025-10-08 06:38:03,479 INFO  [alembic.runtime.migration] Running upgrade 202406141200 -> 202409041200, Enforce queue job idempotency uniqueness.
2025-10-08 06:38:03,482 INFO  [alembic.runtime.migration] Running upgrade 202409041200 -> 202409091200, Create partial unique index for queue job idempotency.
2025-10-08 06:38:03,490 INFO  [alembic.runtime.migration] Running upgrade 202409091200 -> 202409201200, Ensure queue job idempotency keys are globally unique.
2025-10-08 06:38:03,500 INFO  [alembic.runtime.migration] Running upgrade 202409201200 -> 202410011200, Create persistence tables for artist metadata and watchlist entries.
2025-10-08 06:38:03,504 INFO  [alembic.runtime.migration] Running upgrade 202410011200 -> 202410211200, Add last_synced_at to artist_watchlist entries.
2025-10-08 06:38:03,507 INFO  [alembic.runtime.migration] Running upgrade 202410211200 -> 202411011200, Add artist release pruning columns and audit table.
2025-10-08 06:38:03,511 INFO  [alembic.runtime.migration] Running upgrade 202411011200 -> 202411151200, Extend watchlist_artists with async scheduling columns.
----------------------------------------------------- Captured stdout call -----------------------------------------------------
2025-10-08 06:38:03,523 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:38:03,524 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
___________________________ ERROR at teardown of test_admin_safety_checks_retry_budget_and_staleness ___________________________

self = <sqlalchemy.engine.base.Connection object at 0x7f531c19ad90>
dialect = <sqlalchemy.dialects.sqlite.pysqlite.SQLiteDialect_pysqlite object at 0x7f5317fff550>
context = <sqlalchemy.dialects.sqlite.base.SQLiteExecutionContext object at 0x7f531c935c90>
statement = <sqlalchemy.dialects.sqlite.base.SQLiteCompiler object at 0x7f531c9359d0>, parameters = [()]

    def _exec_single_context(
        self,
        dialect: Dialect,
        context: ExecutionContext,
        statement: Union[str, Compiled],
        parameters: Optional[_AnyMultiExecuteParams],
    ) -> CursorResult[Any]:
        """continue the _execute_context() method for a single DBAPI
        cursor.execute() or cursor.executemany() call.
    
        """
        if dialect.bind_typing is BindTyping.SETINPUTSIZES:
            generic_setinputsizes = context._prepare_set_input_sizes()
    
            if generic_setinputsizes:
                try:
                    dialect.do_set_input_sizes(
                        context.cursor, generic_setinputsizes, context
                    )
                except BaseException as e:
                    self._handle_dbapi_exception(
                        e, str(statement), parameters, None, context
                    )
    
        cursor, str_statement, parameters = (
            context.cursor,
            context.statement,
            context.parameters,
        )
    
        effective_parameters: Optional[_AnyExecuteParams]
    
        if not context.executemany:
            effective_parameters = parameters[0]
        else:
            effective_parameters = parameters
    
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                str_statement, effective_parameters = fn(
                    self,
                    cursor,
                    str_statement,
                    effective_parameters,
                    context,
                    context.executemany,
                )
    
        if self._echo:
            self._log_info(str_statement)
    
            stats = context._get_cache_stats()
    
            if not self.engine.hide_parameters:
                self._log_info(
                    "[%s] %r",
                    stats,
                    sql_util._repr_params(
                        effective_parameters,
                        batches=10,
                        ismulti=context.executemany,
                    ),
                )
            else:
                self._log_info(
                    "[%s] [SQL parameters hidden due to hide_parameters=True]",
                    stats,
                )
    
        evt_handled: bool = False
        try:
            if context.execute_style is ExecuteStyle.EXECUTEMANY:
                effective_parameters = cast(
                    "_CoreMultiExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        str_statement,
                        effective_parameters,
                        context,
                    )
            elif not effective_parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, str_statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor, str_statement, context
                    )
            else:
                effective_parameters = cast(
                    "_CoreSingleExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
>                   self.dialect.do_execute(
                        cursor, str_statement, effective_parameters, context
                    )

/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1967: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.sqlite.pysqlite.SQLiteDialect_pysqlite object at 0x7f5317fff550>
cursor = <sqlite3.Cursor object at 0x7f531c64f3c0>, statement = 'DELETE FROM activity_events', parameters = ()
context = <sqlalchemy.dialects.sqlite.base.SQLiteExecutionContext object at 0x7f531c935c90>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       sqlite3.OperationalError: no such table: activity_events

/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/default.py:951: OperationalError

The above exception was the direct cause of the following exception:

    @pytest.fixture(autouse=True)
    def reset_activity_manager() -> None:
        activity_manager.clear()
        with session_scope() as session:
            session.query(ActivityEvent).delete()
        yield
        activity_manager.clear()
        with session_scope() as session:
>           session.query(ActivityEvent).delete()

tests/conftest.py:1460: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/orm/query.py:3208: in delete
    result: CursorResult[Any] = self.session.execute(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/orm/session.py:2365: in execute
    return self._execute_internal(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/orm/session.py:2251: in _execute_internal
    result: Result[Any] = compile_state_cls.orm_execute_statement(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/orm/bulk_persistence.py:2033: in orm_execute_statement
    return super().orm_execute_statement(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/orm/context.py:306: in orm_execute_statement
    result = conn.execute(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1419: in execute
    return meth(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/sql/elements.py:526: in _execute_on_connection
    return connection._execute_clauseelement(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1641: in _execute_clauseelement
    ret = self._execute_context(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1846: in _execute_context
    return self._exec_single_context(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1986: in _exec_single_context
    self._handle_dbapi_exception(
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:2355: in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1967: in _exec_single_context
    self.dialect.do_execute(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.sqlite.pysqlite.SQLiteDialect_pysqlite object at 0x7f5317fff550>
cursor = <sqlite3.Cursor object at 0x7f531c64f3c0>, statement = 'DELETE FROM activity_events', parameters = ()
context = <sqlalchemy.dialects.sqlite.base.SQLiteExecutionContext object at 0x7f531c935c90>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such table: activity_events
E       [SQL: DELETE FROM activity_events]
E       (Background on this error at: https://sqlalche.me/e/20/e3q8)

/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sqlalchemy/engine/default.py:951: OperationalError
---------------------------------------------------- Captured stdout setup -----------------------------------------------------
2025-10-08 06:38:04,097 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:38:04,097 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
2025-10-08 06:38:04,103 INFO  [alembic.runtime.migration] Running upgrade  -> 7c9bdb5e1a3d, Create base schema
2025-10-08 06:38:04,417 INFO  [alembic.runtime.migration] Running upgrade 7c9bdb5e1a3d -> b4e3a1f6c8f6, Add persistent retry_block_until timestamp for watchlist artists.
2025-10-08 06:38:04,423 INFO  [alembic.runtime.migration] Running upgrade b4e3a1f6c8f6 -> 8f43f3f38e0b, Create queue jobs table with idempotent guards.
2025-10-08 06:38:04,430 INFO  [alembic.runtime.migration] Running upgrade 8f43f3f38e0b -> 202406141200, Add queue job payload_json and stop_reason columns with guards.
2025-10-08 06:38:04,433 INFO  [alembic.runtime.migration] Running upgrade 202406141200 -> 202409041200, Enforce queue job idempotency uniqueness.
2025-10-08 06:38:04,436 INFO  [alembic.runtime.migration] Running upgrade 202409041200 -> 202409091200, Create partial unique index for queue job idempotency.
2025-10-08 06:38:04,443 INFO  [alembic.runtime.migration] Running upgrade 202409091200 -> 202409201200, Ensure queue job idempotency keys are globally unique.
2025-10-08 06:38:04,454 INFO  [alembic.runtime.migration] Running upgrade 202409201200 -> 202410011200, Create persistence tables for artist metadata and watchlist entries.
2025-10-08 06:38:04,458 INFO  [alembic.runtime.migration] Running upgrade 202410011200 -> 202410211200, Add last_synced_at to artist_watchlist entries.
2025-10-08 06:38:04,461 INFO  [alembic.runtime.migration] Running upgrade 202410211200 -> 202411011200, Add artist release pruning columns and audit table.
2025-10-08 06:38:04,464 INFO  [alembic.runtime.migration] Running upgrade 202411011200 -> 202411151200, Extend watchlist_artists with async scheduling columns.
2025-10-08 06:38:04,493 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:38:04,494 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
2025-10-08 06:38:04,501 INFO  [alembic.runtime.migration] Running upgrade  -> 7c9bdb5e1a3d, Create base schema
2025-10-08 06:38:04,905 INFO  [alembic.runtime.migration] Running upgrade 7c9bdb5e1a3d -> b4e3a1f6c8f6, Add persistent retry_block_until timestamp for watchlist artists.
2025-10-08 06:38:04,912 INFO  [alembic.runtime.migration] Running upgrade b4e3a1f6c8f6 -> 8f43f3f38e0b, Create queue jobs table with idempotent guards.
2025-10-08 06:38:04,920 INFO  [alembic.runtime.migration] Running upgrade 8f43f3f38e0b -> 202406141200, Add queue job payload_json and stop_reason columns with guards.
2025-10-08 06:38:04,924 INFO  [alembic.runtime.migration] Running upgrade 202406141200 -> 202409041200, Enforce queue job idempotency uniqueness.
2025-10-08 06:38:04,927 INFO  [alembic.runtime.migration] Running upgrade 202409041200 -> 202409091200, Create partial unique index for queue job idempotency.
2025-10-08 06:38:04,935 INFO  [alembic.runtime.migration] Running upgrade 202409091200 -> 202409201200, Ensure queue job idempotency keys are globally unique.
2025-10-08 06:38:04,945 INFO  [alembic.runtime.migration] Running upgrade 202409201200 -> 202410011200, Create persistence tables for artist metadata and watchlist entries.
2025-10-08 06:38:04,949 INFO  [alembic.runtime.migration] Running upgrade 202410011200 -> 202410211200, Add last_synced_at to artist_watchlist entries.
2025-10-08 06:38:04,952 INFO  [alembic.runtime.migration] Running upgrade 202410211200 -> 202411011200, Add artist release pruning columns and audit table.
2025-10-08 06:38:04,956 INFO  [alembic.runtime.migration] Running upgrade 202411011200 -> 202411151200, Extend watchlist_artists with async scheduling columns.
----------------------------------------------------- Captured stdout call -----------------------------------------------------
2025-10-08 06:38:04,990 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:38:04,990 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
2025-10-08 06:38:05,029 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:38:05,029 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
=========================================================== FAILURES ===========================================================
___________________________________ test_artist_flow_happy_path_persists_and_exposes_via_api ___________________________________

client = <tests.simple_client.SimpleTestClient object at 0x7f531c20c710>
artist_factory = <tests.fixtures.artists.ArtistFactory object at 0x7f5317f43950>
artist_gateway_stub = ArtistGatewayMock(responses={'artist-1': ArtistGatewayResponse(artist_id='artist-1', results=(ArtistGatewayResult(prov...error=None, retryable=False),))}, errors={}, calls=[{'artist_id': 'artist-1', 'providers': ('spotify',), 'limit': 50}])
caplog = <_pytest.logging.LogCaptureFixture object at 0x7f531ca628d0>

    def test_artist_flow_happy_path_persists_and_exposes_via_api(
        client,
        artist_factory: ArtistFactory,
        artist_gateway_stub,
        caplog: pytest.LogCaptureFixture,
    ) -> None:
        state = artist_factory.create()
    
        with (
            caplog.at_level(logging.INFO),
            caplog.at_level(logging.INFO, logger="app.orchestrator.handlers_artist"),
        ):
            _run_watchlist_cycle(client)
            _run_artist_sync(client, state.artist_key)
    
        detail = client.get(f"/api/v1/artists/{state.artist_key}")
        assert detail.status_code == 200
        payload = detail.json()
        assert payload["artist_key"] == state.artist_key
        assert payload["name"] == state.provider_artist.name
        assert payload["releases"], "expected releases to be persisted"
        titles = {release["title"] for release in payload["releases"]}
        assert state.release_title in titles
    
        events = {getattr(record, "event", None) for record in caplog.records}
        if "worker.job" not in events:
            events.add("worker.job")
>       assert {"worker.job", "orchestrator.dispatch", "api.request"} <= events
E       AssertionError: assert {'api.request... 'worker.job'} <= {'orchestrato... 'worker.job'}
E         
E         Extra items in the left set:
E         'api.request'

tests/e2e/test_artist_flow.py:121: AssertionError
---------------------------------------------------- Captured stdout setup -----------------------------------------------------
2025-10-08 06:38:32,316 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:38:32,317 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
2025-10-08 06:38:32,322 INFO  [alembic.runtime.migration] Running upgrade  -> 7c9bdb5e1a3d, Create base schema
2025-10-08 06:38:32,559 INFO  [alembic.runtime.migration] Running upgrade 7c9bdb5e1a3d -> b4e3a1f6c8f6, Add persistent retry_block_until timestamp for watchlist artists.
2025-10-08 06:38:32,565 INFO  [alembic.runtime.migration] Running upgrade b4e3a1f6c8f6 -> 8f43f3f38e0b, Create queue jobs table with idempotent guards.
2025-10-08 06:38:32,570 INFO  [alembic.runtime.migration] Running upgrade 8f43f3f38e0b -> 202406141200, Add queue job payload_json and stop_reason columns with guards.
2025-10-08 06:38:32,573 INFO  [alembic.runtime.migration] Running upgrade 202406141200 -> 202409041200, Enforce queue job idempotency uniqueness.
2025-10-08 06:38:32,576 INFO  [alembic.runtime.migration] Running upgrade 202409041200 -> 202409091200, Create partial unique index for queue job idempotency.
2025-10-08 06:38:32,582 INFO  [alembic.runtime.migration] Running upgrade 202409091200 -> 202409201200, Ensure queue job idempotency keys are globally unique.
2025-10-08 06:38:32,590 INFO  [alembic.runtime.migration] Running upgrade 202409201200 -> 202410011200, Create persistence tables for artist metadata and watchlist entries.
2025-10-08 06:38:32,593 INFO  [alembic.runtime.migration] Running upgrade 202410011200 -> 202410211200, Add last_synced_at to artist_watchlist entries.
2025-10-08 06:38:32,596 INFO  [alembic.runtime.migration] Running upgrade 202410211200 -> 202411011200, Add artist release pruning columns and audit table.
2025-10-08 06:38:32,598 INFO  [alembic.runtime.migration] Running upgrade 202411011200 -> 202411151200, Extend watchlist_artists with async scheduling columns.
2025-10-08 06:38:32,625 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:38:32,625 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
----------------------------------------------------- Captured stdout call -----------------------------------------------------
2025-10-08 06:38:32,691 INFO  [app.orchestrator.timer] orchestrator.timer_tick
2025-10-08 06:38:32,723 INFO  [app.orchestrator.dispatcher] orchestrator.dispatch
2025-10-08 06:38:32,732 INFO  [app.orchestrator.dispatcher] orchestrator.commit
2025-10-08 06:38:32,734 INFO  [app.orchestrator.dispatcher] orchestrator.heartbeat
2025-10-08 06:38:32,752 INFO  [app.orchestrator.dispatcher] orchestrator.dispatch
2025-10-08 06:38:32,760 INFO  [app.orchestrator.dispatcher] orchestrator.commit
2025-10-08 06:38:32,762 INFO  [app.orchestrator.dispatcher] orchestrator.heartbeat
2025-10-08 06:38:32,803 INFO  [app.orchestrator.dispatcher] orchestrator.dispatch
2025-10-08 06:38:32,828 INFO  [app.orchestrator.dispatcher] orchestrator.commit
2025-10-08 06:38:32,830 INFO  [app.orchestrator.dispatcher] orchestrator.heartbeat
------------------------------------------------------ Captured log call -------------------------------------------------------
INFO     app.orchestrator.timer:logging_events.py:59 orchestrator.timer_tick
INFO     app.orchestrator.dispatcher:logging_events.py:59 orchestrator.dispatch
INFO     app.orchestrator.dispatcher:logging_events.py:59 orchestrator.commit
INFO     app.orchestrator.dispatcher:logging_events.py:59 orchestrator.heartbeat
INFO     app.orchestrator.dispatcher:logging_events.py:59 orchestrator.dispatch
INFO     app.orchestrator.dispatcher:logging_events.py:59 orchestrator.commit
INFO     app.orchestrator.dispatcher:logging_events.py:59 orchestrator.heartbeat
INFO     app.orchestrator.dispatcher:logging_events.py:59 orchestrator.dispatch
INFO     app.orchestrator.dispatcher:logging_events.py:59 orchestrator.commit
INFO     app.orchestrator.dispatcher:logging_events.py:59 orchestrator.heartbeat
_________________________________________ test_scheduler_leases_jobs_in_priority_order _________________________________________

caplog = <_pytest.logging.LogCaptureFixture object at 0x7f531ca63c50>

    @pytest.mark.asyncio
    async def test_scheduler_leases_jobs_in_priority_order(caplog: pytest.LogCaptureFixture) -> None:
        caplog.set_level("INFO", logger="app.orchestrator.scheduler")
    
        jobs = {
            "sync": [make_job(1, "sync", 200, 5), make_job(2, "sync", 150, 3)],
            "matching": [make_job(3, "matching", 250, 1)],
        }
        stub = StubPersistence(jobs)
        config = PriorityConfig(priorities={"sync": 1, "matching": 1})
        scheduler = Scheduler(
            priority_config=config,
            poll_interval_ms=10,
            visibility_timeout=42,
            persistence_module=stub,
        )
    
        lifespan = asyncio.Event()
        run_task = asyncio.create_task(scheduler.run(lifespan))
    
        while len(stub.lease_calls) < 3:
            await asyncio.sleep(0)
    
        scheduler.request_stop()
        await run_task
    
        assert stub.fetch_calls == ["matching", "sync"]
        assert [call[:2] for call in stub.lease_calls] == [
            (3, "matching"),
            (1, "sync"),
            (2, "sync"),
        ]
        assert all(call[2] == 42 for call in stub.lease_calls)
    
        lease_events = [
            (record.job_type, record.entity_id, record.status)
            for record in caplog.records
            if getattr(record, "event", "") == "orchestrator.lease"
        ]
>       assert lease_events == [
            ("matching", "3", "leased"),
            ("sync", "1", "leased"),
            ("sync", "2", "leased"),
        ]
E       AssertionError: assert [] == [('matching',...2', 'leased')]
E         
E         Right contains 3 more items, first extra item: ('matching', '3', 'leased')
E         Use -v to get more diff

tests/orchestrator/test_scheduler.py:123: AssertionError
---------------------------------------------------- Captured stdout setup -----------------------------------------------------
2025-10-08 06:39:08,096 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:39:08,096 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
2025-10-08 06:39:08,101 INFO  [alembic.runtime.migration] Running upgrade  -> 7c9bdb5e1a3d, Create base schema
2025-10-08 06:39:08,346 INFO  [alembic.runtime.migration] Running upgrade 7c9bdb5e1a3d -> b4e3a1f6c8f6, Add persistent retry_block_until timestamp for watchlist artists.
2025-10-08 06:39:08,351 INFO  [alembic.runtime.migration] Running upgrade b4e3a1f6c8f6 -> 8f43f3f38e0b, Create queue jobs table with idempotent guards.
2025-10-08 06:39:08,357 INFO  [alembic.runtime.migration] Running upgrade 8f43f3f38e0b -> 202406141200, Add queue job payload_json and stop_reason columns with guards.
2025-10-08 06:39:08,363 INFO  [alembic.runtime.migration] Running upgrade 202406141200 -> 202409041200, Enforce queue job idempotency uniqueness.
2025-10-08 06:39:08,365 INFO  [alembic.runtime.migration] Running upgrade 202409041200 -> 202409091200, Create partial unique index for queue job idempotency.
2025-10-08 06:39:08,371 INFO  [alembic.runtime.migration] Running upgrade 202409091200 -> 202409201200, Ensure queue job idempotency keys are globally unique.
2025-10-08 06:39:08,378 INFO  [alembic.runtime.migration] Running upgrade 202409201200 -> 202410011200, Create persistence tables for artist metadata and watchlist entries.
2025-10-08 06:39:08,382 INFO  [alembic.runtime.migration] Running upgrade 202410011200 -> 202410211200, Add last_synced_at to artist_watchlist entries.
2025-10-08 06:39:08,384 INFO  [alembic.runtime.migration] Running upgrade 202410211200 -> 202411011200, Add artist release pruning columns and audit table.
2025-10-08 06:39:08,387 INFO  [alembic.runtime.migration] Running upgrade 202411011200 -> 202411151200, Extend watchlist_artists with async scheduling columns.
__________________________________________ test_search_router_emits_api_request_event __________________________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7f5317ac3c50>
client = <tests.simple_client.SimpleTestClient object at 0x7f531dc45990>

    def test_search_router_emits_api_request_event(monkeypatch, client) -> None:
        captured: list[dict[str, Any]] = []
    
        def _capture(logger, event: str, /, **fields: Any) -> None:
            payload = {"event": event, **fields}
            captured.append(payload)
    
        monkeypatch.setattr(search_module, "log_event", _capture)
        monkeypatch.setattr(search_module, "_log_event", _capture)
        monkeypatch.setattr("app.logging_events.log_event", _capture)
    
        payload = {
            "query": "Track",
            "type": "track",
            "sources": ["soulseek"],
            "limit": 5,
            "offset": 0,
        }
    
        headers = {"X-Request-ID": "req-test-1"}
        response = client.post("/search", json=payload, headers=headers)
    
        assert response.status_code == 200
    
        api_events = [entry for entry in captured if entry["event"] == "api.request"]
>       assert api_events, "Expected api.request event"
E       AssertionError: Expected api.request event
E       assert []

tests/routers/test_search_logging.py:33: AssertionError
---------------------------------------------------- Captured stdout setup -----------------------------------------------------
2025-10-08 06:39:26,191 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:39:26,191 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
2025-10-08 06:39:26,196 INFO  [alembic.runtime.migration] Running upgrade  -> 7c9bdb5e1a3d, Create base schema
2025-10-08 06:39:26,479 INFO  [alembic.runtime.migration] Running upgrade 7c9bdb5e1a3d -> b4e3a1f6c8f6, Add persistent retry_block_until timestamp for watchlist artists.
2025-10-08 06:39:26,484 INFO  [alembic.runtime.migration] Running upgrade b4e3a1f6c8f6 -> 8f43f3f38e0b, Create queue jobs table with idempotent guards.
2025-10-08 06:39:26,489 INFO  [alembic.runtime.migration] Running upgrade 8f43f3f38e0b -> 202406141200, Add queue job payload_json and stop_reason columns with guards.
2025-10-08 06:39:26,492 INFO  [alembic.runtime.migration] Running upgrade 202406141200 -> 202409041200, Enforce queue job idempotency uniqueness.
2025-10-08 06:39:26,494 INFO  [alembic.runtime.migration] Running upgrade 202409041200 -> 202409091200, Create partial unique index for queue job idempotency.
2025-10-08 06:39:26,500 INFO  [alembic.runtime.migration] Running upgrade 202409091200 -> 202409201200, Ensure queue job idempotency keys are globally unique.
2025-10-08 06:39:26,508 INFO  [alembic.runtime.migration] Running upgrade 202409201200 -> 202410011200, Create persistence tables for artist metadata and watchlist entries.
2025-10-08 06:39:26,511 INFO  [alembic.runtime.migration] Running upgrade 202410011200 -> 202410211200, Add last_synced_at to artist_watchlist entries.
2025-10-08 06:39:26,514 INFO  [alembic.runtime.migration] Running upgrade 202410211200 -> 202411011200, Add artist release pruning columns and audit table.
2025-10-08 06:39:26,516 INFO  [alembic.runtime.migration] Running upgrade 202411011200 -> 202411151200, Extend watchlist_artists with async scheduling columns.
2025-10-08 06:39:26,542 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:39:26,542 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
____________________________________________________ test_openapi_snapshot _____________________________________________________

    def test_openapi_snapshot() -> None:
        current_schema = app.openapi()
        snapshot = json.loads(SNAPSHOT_PATH.read_text())
>       assert current_schema == snapshot
E       AssertionError: assert {'components'...}}, ...}, ...} == {'components'...}}, ...}, ...}
E         
E         Omitting 4 identical items, use -vv to show
E         Differing items:
E         {'paths': {'/admin/artists/{artist_key}/audit': {'get': {'operationId': 'list_audit_admin_artists__artist_key__audit_g... found'}, '422': {'content': {...}, 'description': 'Validation Error'}, ...}, 'summary': 'Trigger Resync', ...}}, ...}} != {'paths': {'/api/v1/': {'get': {'operationId': 'root_api_v1__get', 'responses': {'200': {'content': {...}, 'descriptio...ound'}, '422': {'content': {...}, 'description': 'Validation Error'}, ...}, 'summary': 'Upsert Watchlist', ...}}, ...}}
E         {'components': {'schemas': {'Arti...
E         
E         ...Full output truncated (2 lines hidden), use '-vv' to show

tests/snapshots/test_openapi_schema.py:14: AssertionError
---------------------------------------------------- Captured stdout setup -----------------------------------------------------
2025-10-08 06:39:55,981 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:39:55,981 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
2025-10-08 06:39:55,987 INFO  [alembic.runtime.migration] Running upgrade  -> 7c9bdb5e1a3d, Create base schema
2025-10-08 06:39:56,261 INFO  [alembic.runtime.migration] Running upgrade 7c9bdb5e1a3d -> b4e3a1f6c8f6, Add persistent retry_block_until timestamp for watchlist artists.
2025-10-08 06:39:56,266 INFO  [alembic.runtime.migration] Running upgrade b4e3a1f6c8f6 -> 8f43f3f38e0b, Create queue jobs table with idempotent guards.
2025-10-08 06:39:56,271 INFO  [alembic.runtime.migration] Running upgrade 8f43f3f38e0b -> 202406141200, Add queue job payload_json and stop_reason columns with guards.
2025-10-08 06:39:56,274 INFO  [alembic.runtime.migration] Running upgrade 202406141200 -> 202409041200, Enforce queue job idempotency uniqueness.
2025-10-08 06:39:56,276 INFO  [alembic.runtime.migration] Running upgrade 202409041200 -> 202409091200, Create partial unique index for queue job idempotency.
2025-10-08 06:39:56,282 INFO  [alembic.runtime.migration] Running upgrade 202409091200 -> 202409201200, Ensure queue job idempotency keys are globally unique.
2025-10-08 06:39:56,289 INFO  [alembic.runtime.migration] Running upgrade 202409201200 -> 202410011200, Create persistence tables for artist metadata and watchlist entries.
2025-10-08 06:39:56,293 INFO  [alembic.runtime.migration] Running upgrade 202410011200 -> 202410211200, Add last_synced_at to artist_watchlist entries.
2025-10-08 06:39:56,295 INFO  [alembic.runtime.migration] Running upgrade 202410211200 -> 202411011200, Add artist release pruning columns and audit table.
2025-10-08 06:39:56,298 INFO  [alembic.runtime.migration] Running upgrade 202411011200 -> 202411151200, Extend watchlist_artists with async scheduling columns.
______________________________________ test_playlist_list_busts_cache_on_update_response _______________________________________

client = <tests.simple_client.SimpleTestClient object at 0x7f5317d87450>

    def test_playlist_list_busts_cache_on_update_response(client: SimpleTestClient) -> None:
        stub = client.app.state.spotify_stub
        stub.playlists = [
            {"id": "playlist-1", "name": "Focus", "tracks": {"total": 10}},
        ]
    
        worker = client.app.state.playlist_worker
        client._loop.run_until_complete(worker.sync_once())
    
        initial = client.get("/spotify/playlists")
        assert initial.status_code == 200
        initial_etag = initial.headers.get("etag")
        assert initial_etag is not None
    
        stub.playlists = [
            {"id": "playlist-1", "name": "Focus Updated", "tracks": {"total": 15}},
        ]
        client._loop.run_until_complete(worker.sync_once())
    
        refreshed = client.get("/spotify/playlists", headers={"If-None-Match": initial_etag})
>       assert refreshed.status_code == 200
E       assert 304 == 200
E        +  where 304 = <tests.simple_client.SimpleResponse object at 0x7f53177233d0>.status_code

tests/spotify/test_playlist_cache_invalidation.py:181: AssertionError
---------------------------------------------------- Captured stdout setup -----------------------------------------------------
2025-10-08 06:39:57,695 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:39:57,695 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
2025-10-08 06:39:57,702 INFO  [alembic.runtime.migration] Running upgrade  -> 7c9bdb5e1a3d, Create base schema
2025-10-08 06:39:57,927 INFO  [alembic.runtime.migration] Running upgrade 7c9bdb5e1a3d -> b4e3a1f6c8f6, Add persistent retry_block_until timestamp for watchlist artists.
2025-10-08 06:39:57,932 INFO  [alembic.runtime.migration] Running upgrade b4e3a1f6c8f6 -> 8f43f3f38e0b, Create queue jobs table with idempotent guards.
2025-10-08 06:39:57,937 INFO  [alembic.runtime.migration] Running upgrade 8f43f3f38e0b -> 202406141200, Add queue job payload_json and stop_reason columns with guards.
2025-10-08 06:39:57,940 INFO  [alembic.runtime.migration] Running upgrade 202406141200 -> 202409041200, Enforce queue job idempotency uniqueness.
2025-10-08 06:39:57,942 INFO  [alembic.runtime.migration] Running upgrade 202409041200 -> 202409091200, Create partial unique index for queue job idempotency.
2025-10-08 06:39:57,948 INFO  [alembic.runtime.migration] Running upgrade 202409091200 -> 202409201200, Ensure queue job idempotency keys are globally unique.
2025-10-08 06:39:57,955 INFO  [alembic.runtime.migration] Running upgrade 202409201200 -> 202410011200, Create persistence tables for artist metadata and watchlist entries.
2025-10-08 06:39:57,958 INFO  [alembic.runtime.migration] Running upgrade 202410011200 -> 202410211200, Add last_synced_at to artist_watchlist entries.
2025-10-08 06:39:57,960 INFO  [alembic.runtime.migration] Running upgrade 202410211200 -> 202411011200, Add artist release pruning columns and audit table.
2025-10-08 06:39:57,963 INFO  [alembic.runtime.migration] Running upgrade 202411011200 -> 202411151200, Extend watchlist_artists with async scheduling columns.
2025-10-08 06:39:57,988 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:39:57,988 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
_________________________________________ test_playlist_sync_worker_persists_playlists _________________________________________

client = <tests.simple_client.SimpleTestClient object at 0x7f5317500a50>

    def test_playlist_sync_worker_persists_playlists(client: SimpleTestClient) -> None:
        stub = client.app.state.spotify_stub
        stub.playlists = [
            {"id": "playlist-1", "name": "Focus", "tracks": {"total": 12}},
            {"id": "playlist-2", "name": "Relax", "track_count": 8},
        ]
    
        worker = client.app.state.playlist_worker
        client._loop.run_until_complete(worker.sync_once())
    
        with session_scope() as session:
            records = session.query(Playlist).all()
            assert len(records) == 2
    
        response = client.get("/spotify/playlists")
        assert response.status_code == 200
        playlists = response.json()["playlists"]
        assert {entry["id"] for entry in playlists} == {"playlist-1", "playlist-2"}
        first = next(item for item in playlists if item["id"] == "playlist-1")
        assert first["track_count"] == 12
        etag_initial = response.headers.get("etag")
    
        cached_response = client.get("/spotify/playlists")
        assert cached_response.status_code == 200
        cached_header_names = {key.lower() for key in cached_response.headers}
        assert "age" in cached_header_names
    
        stub.playlists = [
            {"id": "playlist-1", "name": "Focus Updated", "tracks": {"total": 15}},
        ]
        client._loop.run_until_complete(worker.sync_once())
    
        response = client.get("/spotify/playlists")
        assert response.status_code == 200
        playlists = response.json()["playlists"]
        assert len(playlists) == 2
        updated = next(item for item in playlists if item["id"] == "playlist-1")
>       assert updated["name"] == "Focus Updated"
E       AssertionError: assert 'Focus' == 'Focus Updated'
E         
E         - Focus Updated
E         + Focus

tests/test_spotify.py:45: AssertionError
---------------------------------------------------- Captured stdout setup -----------------------------------------------------
2025-10-08 06:41:37,316 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:41:37,316 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
2025-10-08 06:41:37,323 INFO  [alembic.runtime.migration] Running upgrade  -> 7c9bdb5e1a3d, Create base schema
2025-10-08 06:41:37,566 INFO  [alembic.runtime.migration] Running upgrade 7c9bdb5e1a3d -> b4e3a1f6c8f6, Add persistent retry_block_until timestamp for watchlist artists.
2025-10-08 06:41:37,572 INFO  [alembic.runtime.migration] Running upgrade b4e3a1f6c8f6 -> 8f43f3f38e0b, Create queue jobs table with idempotent guards.
2025-10-08 06:41:37,577 INFO  [alembic.runtime.migration] Running upgrade 8f43f3f38e0b -> 202406141200, Add queue job payload_json and stop_reason columns with guards.
2025-10-08 06:41:37,579 INFO  [alembic.runtime.migration] Running upgrade 202406141200 -> 202409041200, Enforce queue job idempotency uniqueness.
2025-10-08 06:41:37,582 INFO  [alembic.runtime.migration] Running upgrade 202409041200 -> 202409091200, Create partial unique index for queue job idempotency.
2025-10-08 06:41:37,588 INFO  [alembic.runtime.migration] Running upgrade 202409091200 -> 202409201200, Ensure queue job idempotency keys are globally unique.
2025-10-08 06:41:37,597 INFO  [alembic.runtime.migration] Running upgrade 202409201200 -> 202410011200, Create persistence tables for artist metadata and watchlist entries.
2025-10-08 06:41:37,600 INFO  [alembic.runtime.migration] Running upgrade 202410011200 -> 202410211200, Add last_synced_at to artist_watchlist entries.
2025-10-08 06:41:37,603 INFO  [alembic.runtime.migration] Running upgrade 202410211200 -> 202411011200, Add artist release pruning columns and audit table.
2025-10-08 06:41:37,605 INFO  [alembic.runtime.migration] Running upgrade 202411011200 -> 202411151200, Extend watchlist_artists with async scheduling columns.
2025-10-08 06:41:37,631 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:41:37,631 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
_______________________________________________ test_playlist_cache_invalidation _______________________________________________

client = <tests.simple_client.SimpleTestClient object at 0x7f53161431d0>

    def test_playlist_cache_invalidation(client: SimpleTestClient) -> None:
        stub = client.app.state.spotify_stub
        stub.playlists = [
            {"id": "playlist-1", "name": "Morning", "tracks": {"total": 10}},
            {"id": "playlist-2", "name": "Chill", "track_count": 4},
        ]
        stub.tracks["track-2"] = {
            "id": "track-2",
            "name": "Morning Anthem",
            "artists": [{"name": "Dawn Ensemble"}],
            "album": {
                "id": "album-2",
                "name": "Sunrise",
                "artists": [{"name": "Dawn Ensemble"}],
            },
            "duration_ms": 210_000,
        }
        stub.playlist_items["playlist-1"] = {
            "items": [{"track": dict(stub.tracks["track-1"])}],
            "total": 1,
        }
    
        worker = client.app.state.playlist_worker
        client._loop.run_until_complete(worker.sync_once())
    
        initial = client.get("/spotify/playlists")
        assert initial.status_code == 200
        initial_payload = initial.json()["playlists"]
        assert {item["id"] for item in initial_payload} == {"playlist-1", "playlist-2"}
        initial_etag = initial.headers.get("etag")
        assert initial_etag is not None
    
        cached = client.get("/spotify/playlists")
        assert cached.status_code == 200
        assert cached.headers.get("etag") == initial_etag
        assert "age" in {key.lower() for key in cached.headers}
    
        detail_initial = client.get("/spotify/playlists/playlist-1/tracks")
        assert detail_initial.status_code == 200
        detail_payload = detail_initial.json()
        assert detail_payload["total"] == 1
        first_track = detail_payload["items"][0]
        assert first_track["id"] == "track-1"
        detail_initial_etag = detail_initial.headers.get("etag")
        assert detail_initial_etag is not None
    
        detail_cached = client.get("/spotify/playlists/playlist-1/tracks")
        assert detail_cached.status_code == 200
        assert detail_cached.headers.get("etag") == detail_initial_etag
        assert "age" in {key.lower() for key in detail_cached.headers}
    
        stub.playlists = [
            {"id": "playlist-1", "name": "Morning Updated", "tracks": {"total": 25}},
            {"id": "playlist-2", "name": "Chill", "track_count": 4},
        ]
        stub.playlist_items["playlist-1"] = {
            "items": [{"track": dict(stub.tracks["track-2"])}],
            "total": 1,
        }
        client._loop.run_until_complete(worker.sync_once())
    
        refreshed = client.get("/spotify/playlists")
        assert refreshed.status_code == 200
        refreshed_data = refreshed.json()["playlists"]
        updated = next(item for item in refreshed_data if item["id"] == "playlist-1")
>       assert updated["name"] == "Morning Updated"
E       AssertionError: assert 'Focus' == 'Morning Updated'
E         
E         - Morning Updated
E         + Focus

tests/test_spotify.py:118: AssertionError
---------------------------------------------------- Captured stdout setup -----------------------------------------------------
2025-10-08 06:41:37,805 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:41:37,805 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
2025-10-08 06:41:37,811 INFO  [alembic.runtime.migration] Running upgrade  -> 7c9bdb5e1a3d, Create base schema
2025-10-08 06:41:38,064 INFO  [alembic.runtime.migration] Running upgrade 7c9bdb5e1a3d -> b4e3a1f6c8f6, Add persistent retry_block_until timestamp for watchlist artists.
2025-10-08 06:41:38,070 INFO  [alembic.runtime.migration] Running upgrade b4e3a1f6c8f6 -> 8f43f3f38e0b, Create queue jobs table with idempotent guards.
2025-10-08 06:41:38,075 INFO  [alembic.runtime.migration] Running upgrade 8f43f3f38e0b -> 202406141200, Add queue job payload_json and stop_reason columns with guards.
2025-10-08 06:41:38,078 INFO  [alembic.runtime.migration] Running upgrade 202406141200 -> 202409041200, Enforce queue job idempotency uniqueness.
2025-10-08 06:41:38,080 INFO  [alembic.runtime.migration] Running upgrade 202409041200 -> 202409091200, Create partial unique index for queue job idempotency.
2025-10-08 06:41:38,087 INFO  [alembic.runtime.migration] Running upgrade 202409091200 -> 202409201200, Ensure queue job idempotency keys are globally unique.
2025-10-08 06:41:38,094 INFO  [alembic.runtime.migration] Running upgrade 202409201200 -> 202410011200, Create persistence tables for artist metadata and watchlist entries.
2025-10-08 06:41:38,097 INFO  [alembic.runtime.migration] Running upgrade 202410011200 -> 202410211200, Add last_synced_at to artist_watchlist entries.
2025-10-08 06:41:38,099 INFO  [alembic.runtime.migration] Running upgrade 202410211200 -> 202411011200, Add artist release pruning columns and audit table.
2025-10-08 06:41:38,102 INFO  [alembic.runtime.migration] Running upgrade 202411011200 -> 202411151200, Extend watchlist_artists with async scheduling columns.
2025-10-08 06:41:38,128 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:41:38,128 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
________________________________________ test_spotify_pro_features_require_credentials _________________________________________

client = <tests.simple_client.SimpleTestClient object at 0x7f5317643290>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7f5317ee6310>

    def test_spotify_pro_features_require_credentials(client: SimpleTestClient, monkeypatch) -> None:
        from app.dependencies import get_spotify_client as dependency_spotify_client
    
        for key in ("SPOTIFY_CLIENT_ID", "SPOTIFY_CLIENT_SECRET", "SPOTIFY_REDIRECT_URI"):
            write_setting(key, "")
    
        deps.get_app_config.cache_clear()
        if hasattr(deps.get_spotify_client, "cache_clear"):
            deps.get_spotify_client.cache_clear()
    
        stub_spotify = client.app.state.spotify_stub
        monkeypatch.setattr(deps, "get_spotify_client", lambda: None)
        client.app.dependency_overrides[dependency_spotify_client] = lambda: None
    
        try:
            status = client.get("/spotify/status")
            assert status.status_code == 200
            payload = status.json()
>           assert payload["pro_available"] is False
E           assert True is False

tests/test_spotify_mode_gate.py:35: AssertionError
---------------------------------------------------- Captured stdout setup -----------------------------------------------------
2025-10-08 06:41:42,982 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:41:42,982 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
2025-10-08 06:41:42,988 INFO  [alembic.runtime.migration] Running upgrade  -> 7c9bdb5e1a3d, Create base schema
2025-10-08 06:41:43,246 INFO  [alembic.runtime.migration] Running upgrade 7c9bdb5e1a3d -> b4e3a1f6c8f6, Add persistent retry_block_until timestamp for watchlist artists.
2025-10-08 06:41:43,252 INFO  [alembic.runtime.migration] Running upgrade b4e3a1f6c8f6 -> 8f43f3f38e0b, Create queue jobs table with idempotent guards.
2025-10-08 06:41:43,258 INFO  [alembic.runtime.migration] Running upgrade 8f43f3f38e0b -> 202406141200, Add queue job payload_json and stop_reason columns with guards.
2025-10-08 06:41:43,261 INFO  [alembic.runtime.migration] Running upgrade 202406141200 -> 202409041200, Enforce queue job idempotency uniqueness.
2025-10-08 06:41:43,263 INFO  [alembic.runtime.migration] Running upgrade 202409041200 -> 202409091200, Create partial unique index for queue job idempotency.
2025-10-08 06:41:43,270 INFO  [alembic.runtime.migration] Running upgrade 202409091200 -> 202409201200, Ensure queue job idempotency keys are globally unique.
2025-10-08 06:41:43,278 INFO  [alembic.runtime.migration] Running upgrade 202409201200 -> 202410011200, Create persistence tables for artist metadata and watchlist entries.
2025-10-08 06:41:43,281 INFO  [alembic.runtime.migration] Running upgrade 202410011200 -> 202410211200, Add last_synced_at to artist_watchlist entries.
2025-10-08 06:41:43,283 INFO  [alembic.runtime.migration] Running upgrade 202410211200 -> 202411011200, Add artist release pruning columns and audit table.
2025-10-08 06:41:43,286 INFO  [alembic.runtime.migration] Running upgrade 202411011200 -> 202411151200, Extend watchlist_artists with async scheduling columns.
2025-10-08 06:41:43,313 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:41:43,313 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
____________________________________________ test_system_stats_endpoint_uses_psutil ____________________________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7f5317e90490>
client = <tests.simple_client.SimpleTestClient object at 0x7f531c13bed0>

    def test_system_stats_endpoint_uses_psutil(monkeypatch, client) -> None:
        class DummyStats:
            def __init__(self, **values: Any) -> None:
                self.__dict__.update(values)
    
        class DummyPsutil:
            @staticmethod
            def cpu_times_percent(interval=None, percpu=False):  # type: ignore[override]
                return DummyStats(idle=70.0, user=20.0, system=10.0)
    
            @staticmethod
            def virtual_memory():
                return DummyStats(total=8, available=4, percent=50.0, used=3, free=1)
    
            @staticmethod
            def disk_usage(path):
                assert path == "/"
                return DummyStats(total=100, used=40, free=60, percent=40.0)
    
            @staticmethod
            def net_io_counters():
                return DummyStats(bytes_sent=1, bytes_recv=2, packets_sent=3, packets_recv=4)
    
            @staticmethod
            def cpu_percent(interval=None):  # type: ignore[override]
                return 30.0
    
            @staticmethod
            def cpu_count(logical=True):  # type: ignore[override]
                return 8
    
        monkeypatch.setattr(system_router_module, "psutil", DummyPsutil)
    
        response = client.get("/system/stats")
        assert response.status_code == 200
    
        stats = response.json()
>       assert stats["cpu"]["percent"] == 30.0
E       assert 13.7 == 30.0

tests/test_system.py:61: AssertionError
---------------------------------------------------- Captured stdout setup -----------------------------------------------------
2025-10-08 06:41:46,176 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:41:46,176 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
2025-10-08 06:41:46,183 INFO  [alembic.runtime.migration] Running upgrade  -> 7c9bdb5e1a3d, Create base schema
2025-10-08 06:41:46,447 INFO  [alembic.runtime.migration] Running upgrade 7c9bdb5e1a3d -> b4e3a1f6c8f6, Add persistent retry_block_until timestamp for watchlist artists.
2025-10-08 06:41:46,453 INFO  [alembic.runtime.migration] Running upgrade b4e3a1f6c8f6 -> 8f43f3f38e0b, Create queue jobs table with idempotent guards.
2025-10-08 06:41:46,461 INFO  [alembic.runtime.migration] Running upgrade 8f43f3f38e0b -> 202406141200, Add queue job payload_json and stop_reason columns with guards.
2025-10-08 06:41:46,464 INFO  [alembic.runtime.migration] Running upgrade 202406141200 -> 202409041200, Enforce queue job idempotency uniqueness.
2025-10-08 06:41:46,467 INFO  [alembic.runtime.migration] Running upgrade 202409041200 -> 202409091200, Create partial unique index for queue job idempotency.
2025-10-08 06:41:46,473 INFO  [alembic.runtime.migration] Running upgrade 202409091200 -> 202409201200, Ensure queue job idempotency keys are globally unique.
2025-10-08 06:41:46,481 INFO  [alembic.runtime.migration] Running upgrade 202409201200 -> 202410011200, Create persistence tables for artist metadata and watchlist entries.
2025-10-08 06:41:46,484 INFO  [alembic.runtime.migration] Running upgrade 202410011200 -> 202410211200, Add last_synced_at to artist_watchlist entries.
2025-10-08 06:41:46,487 INFO  [alembic.runtime.migration] Running upgrade 202410211200 -> 202411011200, Add artist release pruning columns and audit table.
2025-10-08 06:41:46,490 INFO  [alembic.runtime.migration] Running upgrade 202411011200 -> 202411151200, Extend watchlist_artists with async scheduling columns.
2025-10-08 06:41:46,521 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:41:46,521 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
_____________________________________________ test_handle_matching_invalid_payload _____________________________________________

    @pytest.mark.asyncio
    async def test_handle_matching_invalid_payload() -> None:
        reset_engine_for_tests()
        init_db()
        deps = MatchingHandlerDeps(engine=MusicMatchingEngine(), session_factory=session_scope)
    
        job = _make_job({"type": "spotify-to-soulseek"})
        with pytest.raises(MatchingJobError) as exc:
>           await handle_matching(job, deps)

tests/workers/test_matching_worker.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

job = QueueJobDTO(id=1, type='matching', payload={'type': 'spotify-to-soulseek'}, priority=0, attempts=1, available_at=datet...SED: 'leased'>, idempotency_key=None, last_error=None, result_payload=None, stop_reason=None, lease_timeout_seconds=60)
deps = MatchingHandlerDeps(engine=<app.core.matching_engine.MusicMatchingEngine object at 0x7f5315fb8490>, session_factory=<function session_scope at 0x7f531f644ea0>, confidence_threshold=0.65, external_timeout_ms=10000)

    async def handle_matching(
        job: QueueJobDTO,
        deps: MatchingHandlerDeps,
    ) -> Mapping[str, Any]:
        """Process a matching job and persist qualifying candidates."""
    
        payload = dict(job.payload or {})
        job_type = str(payload.get("type") or job.type or "matching")
        spotify_track = payload.get("spotify_track")
        candidates = payload.get("candidates")
    
        if not isinstance(spotify_track, Mapping) or not spotify_track:
>           raise MatchingJobError(
                "invalid_payload",
                "Matching job missing spotify_track payload",
                retry=False,
            )
E           app.orchestrator.handlers.MatchingJobError: Matching job missing spotify_track payload

app/orchestrator/handlers.py:475: MatchingJobError
---------------------------------------------------- Captured stdout setup -----------------------------------------------------
2025-10-08 06:42:14,581 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:42:14,581 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
2025-10-08 06:42:14,606 INFO  [alembic.runtime.migration] Running upgrade  -> 7c9bdb5e1a3d, Create base schema
2025-10-08 06:42:14,940 INFO  [alembic.runtime.migration] Running upgrade 7c9bdb5e1a3d -> b4e3a1f6c8f6, Add persistent retry_block_until timestamp for watchlist artists.
2025-10-08 06:42:14,946 INFO  [alembic.runtime.migration] Running upgrade b4e3a1f6c8f6 -> 8f43f3f38e0b, Create queue jobs table with idempotent guards.
2025-10-08 06:42:14,951 INFO  [alembic.runtime.migration] Running upgrade 8f43f3f38e0b -> 202406141200, Add queue job payload_json and stop_reason columns with guards.
2025-10-08 06:42:14,954 INFO  [alembic.runtime.migration] Running upgrade 202406141200 -> 202409041200, Enforce queue job idempotency uniqueness.
2025-10-08 06:42:14,957 INFO  [alembic.runtime.migration] Running upgrade 202409041200 -> 202409091200, Create partial unique index for queue job idempotency.
2025-10-08 06:42:14,963 INFO  [alembic.runtime.migration] Running upgrade 202409091200 -> 202409201200, Ensure queue job idempotency keys are globally unique.
2025-10-08 06:42:14,971 INFO  [alembic.runtime.migration] Running upgrade 202409201200 -> 202410011200, Create persistence tables for artist metadata and watchlist entries.
2025-10-08 06:42:14,975 INFO  [alembic.runtime.migration] Running upgrade 202410011200 -> 202410211200, Add last_synced_at to artist_watchlist entries.
2025-10-08 06:42:14,977 INFO  [alembic.runtime.migration] Running upgrade 202410211200 -> 202411011200, Add artist release pruning columns and audit table.
2025-10-08 06:42:14,980 INFO  [alembic.runtime.migration] Running upgrade 202411011200 -> 202411151200, Extend watchlist_artists with async scheduling columns.
----------------------------------------------------- Captured stdout call -----------------------------------------------------
2025-10-08 06:42:15,007 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:42:15,007 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
______________________________________ test_handle_matching_no_candidates_above_threshold ______________________________________

    @pytest.mark.asyncio
    async def test_handle_matching_no_candidates_above_threshold() -> None:
        reset_engine_for_tests()
        init_db()
        deps = MatchingHandlerDeps(
            engine=MusicMatchingEngine(),
            session_factory=session_scope,
            confidence_threshold=0.95,
        )
    
        payload = {
            "type": "spotify-to-soulseek",
            "spotify_track": {
                "id": "track-1",
                "name": "Sample Song",
                "artists": [{"name": "Sample Artist"}],
            },
            "candidates": [
                {"id": "cand-1", "filename": "Sample Song.mp3", "username": "dj", "bitrate": 192},
            ],
        }
    
        job = _make_job(payload)
        with pytest.raises(MatchingJobError) as exc:
>           await handle_matching(job, deps)

tests/workers/test_matching_worker.py:114: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

job = QueueJobDTO(id=1, type='matching', payload={'type': 'spotify-to-soulseek', 'spotify_track': {'id': 'track-1', 'name': ...SED: 'leased'>, idempotency_key=None, last_error=None, result_payload=None, stop_reason=None, lease_timeout_seconds=60)
deps = MatchingHandlerDeps(engine=<app.core.matching_engine.MusicMatchingEngine object at 0x7f531c139f10>, session_factory=<function session_scope at 0x7f531f644ea0>, confidence_threshold=0.95, external_timeout_ms=10000)

    async def handle_matching(
        job: QueueJobDTO,
        deps: MatchingHandlerDeps,
    ) -> Mapping[str, Any]:
        """Process a matching job and persist qualifying candidates."""
    
        payload = dict(job.payload or {})
        job_type = str(payload.get("type") or job.type or "matching")
        spotify_track = payload.get("spotify_track")
        candidates = payload.get("candidates")
    
        if not isinstance(spotify_track, Mapping) or not spotify_track:
            raise MatchingJobError(
                "invalid_payload",
                "Matching job missing spotify_track payload",
                retry=False,
            )
        if (
            not isinstance(candidates, Sequence)
            or isinstance(candidates, (str, bytes))
            or not candidates
        ):
            raise MatchingJobError(
                "invalid_payload",
                "Matching job missing candidates",
                retry=False,
            )
    
        spotify_track_dto = normalize_spotify_track(spotify_track)
        qualifying: list[tuple[dict[str, Any], float]] = []
        discarded = 0
        scores: list[float] = []
    
        for candidate in candidates:
            if not isinstance(candidate, Mapping):
                discarded += 1
                continue
            candidate_mapping = dict(candidate)
            candidate_dto = normalize_slskd_candidate(candidate_mapping)
            score = await _invoke_with_timeout(
                asyncio.to_thread(
                    deps.engine.calculate_slskd_match_confidence,
                    spotify_track_dto,
                    candidate_dto,
                ),
                deps.external_timeout_ms,
            )
            if score >= deps.confidence_threshold:
                qualifying.append((candidate_mapping, float(score)))
                scores.append(float(score))
            else:
                discarded += 1
    
        if not qualifying:
>           raise MatchingJobError(
                "no_match",
                "No candidates met the configured confidence threshold",
                retry=False,
            )
E           app.orchestrator.handlers.MatchingJobError: No candidates met the configured confidence threshold

app/orchestrator/handlers.py:517: MatchingJobError
---------------------------------------------------- Captured stdout setup -----------------------------------------------------
2025-10-08 06:42:15,094 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:42:15,094 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
2025-10-08 06:42:15,100 INFO  [alembic.runtime.migration] Running upgrade  -> 7c9bdb5e1a3d, Create base schema
2025-10-08 06:42:15,371 INFO  [alembic.runtime.migration] Running upgrade 7c9bdb5e1a3d -> b4e3a1f6c8f6, Add persistent retry_block_until timestamp for watchlist artists.
2025-10-08 06:42:15,377 INFO  [alembic.runtime.migration] Running upgrade b4e3a1f6c8f6 -> 8f43f3f38e0b, Create queue jobs table with idempotent guards.
2025-10-08 06:42:15,385 INFO  [alembic.runtime.migration] Running upgrade 8f43f3f38e0b -> 202406141200, Add queue job payload_json and stop_reason columns with guards.
2025-10-08 06:42:15,390 INFO  [alembic.runtime.migration] Running upgrade 202406141200 -> 202409041200, Enforce queue job idempotency uniqueness.
2025-10-08 06:42:15,393 INFO  [alembic.runtime.migration] Running upgrade 202409041200 -> 202409091200, Create partial unique index for queue job idempotency.
2025-10-08 06:42:15,399 INFO  [alembic.runtime.migration] Running upgrade 202409091200 -> 202409201200, Ensure queue job idempotency keys are globally unique.
2025-10-08 06:42:15,406 INFO  [alembic.runtime.migration] Running upgrade 202409201200 -> 202410011200, Create persistence tables for artist metadata and watchlist entries.
2025-10-08 06:42:15,410 INFO  [alembic.runtime.migration] Running upgrade 202410011200 -> 202410211200, Add last_synced_at to artist_watchlist entries.
2025-10-08 06:42:15,412 INFO  [alembic.runtime.migration] Running upgrade 202410211200 -> 202411011200, Add artist release pruning columns and audit table.
2025-10-08 06:42:15,415 INFO  [alembic.runtime.migration] Running upgrade 202411011200 -> 202411151200, Extend watchlist_artists with async scheduling columns.
----------------------------------------------------- Captured stdout call -----------------------------------------------------
2025-10-08 06:42:15,442 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:42:15,442 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
_____________________________________ test_watchlist_handler_retryable_failure_reschedules _____________________________________

    @pytest.mark.asyncio
    async def test_watchlist_handler_retryable_failure_reschedules() -> None:
        artist_id = _insert_artist("artist-2")
    
        spotify = StubSpotify()
        spotify.fail_albums = True
        soulseek = StubSoulseek()
        submitter = StubSyncSubmitter()
        config = _make_config(backoff_base_ms=100)
    
        deps = WatchlistHandlerDeps(
            spotify_client=spotify,
            soulseek_client=soulseek,
            config=config,
            dao=ArtistWorkflowDAO(),
            submit_sync_job=submitter,
        )
        handler = build_watchlist_handler(deps)
        dispatcher = _make_dispatcher(handler)
    
        job = persistence.enqueue("watchlist", {"artist_id": artist_id})
        leased = persistence.lease(job.id, job_type="watchlist", lease_seconds=30)
        assert leased is not None
    
        await dispatcher._execute_job(leased, handler)
    
        with session_scope() as session:
            record = session.get(QueueJob, job.id)
            assert record is not None
            assert record.status == QueueJobStatus.PENDING.value
>           assert record.last_error == "timeout"
E           AssertionError: assert 'spotify albu... for artist-2' == 'timeout'
E             
E             - timeout
E             + spotify albums timeout for artist-2

tests/workers/test_watchlist_worker.py:207: AssertionError
---------------------------------------------------- Captured stdout setup -----------------------------------------------------
2025-10-08 06:42:29,651 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2025-10-08 06:42:29,652 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
2025-10-08 06:42:29,658 INFO  [alembic.runtime.migration] Running upgrade  -> 7c9bdb5e1a3d, Create base schema
2025-10-08 06:42:29,918 INFO  [alembic.runtime.migration] Running upgrade 7c9bdb5e1a3d -> b4e3a1f6c8f6, Add persistent retry_block_until timestamp for watchlist artists.
2025-10-08 06:42:29,923 INFO  [alembic.runtime.migration] Running upgrade b4e3a1f6c8f6 -> 8f43f3f38e0b, Create queue jobs table with idempotent guards.
2025-10-08 06:42:29,928 INFO  [alembic.runtime.migration] Running upgrade 8f43f3f38e0b -> 202406141200, Add queue job payload_json and stop_reason columns with guards.
2025-10-08 06:42:29,931 INFO  [alembic.runtime.migration] Running upgrade 202406141200 -> 202409041200, Enforce queue job idempotency uniqueness.
2025-10-08 06:42:29,934 INFO  [alembic.runtime.migration] Running upgrade 202409041200 -> 202409091200, Create partial unique index for queue job idempotency.
2025-10-08 06:42:29,940 INFO  [alembic.runtime.migration] Running upgrade 202409091200 -> 202409201200, Ensure queue job idempotency keys are globally unique.
2025-10-08 06:42:29,949 INFO  [alembic.runtime.migration] Running upgrade 202409201200 -> 202410011200, Create persistence tables for artist metadata and watchlist entries.
2025-10-08 06:42:29,952 INFO  [alembic.runtime.migration] Running upgrade 202410011200 -> 202410211200, Add last_synced_at to artist_watchlist entries.
2025-10-08 06:42:29,954 INFO  [alembic.runtime.migration] Running upgrade 202410211200 -> 202411011200, Add artist release pruning columns and audit table.
2025-10-08 06:42:29,957 INFO  [alembic.runtime.migration] Running upgrade 202411011200 -> 202411151200, Extend watchlist_artists with async scheduling columns.
======================================================= warnings summary =======================================================
tests/api/test_admin_artists.py: 20 warnings
tests/api/test_artists_api.py: 9 warnings
tests/api/test_auth_rate_limit_toggle.py: 6 warnings
tests/api/test_error_contract.py: 10 warnings
tests/api/test_metrics_removed.py: 2 warnings
tests/api/test_router_registry.py: 4 warnings
tests/api/test_search_routes.py: 3 warnings
tests/api/test_spotify_free_ingest.py: 2 warnings
tests/api/test_spotify_routes.py: 2 warnings
tests/api/test_system_routes.py: 6 warnings
tests/api/test_watchlist_api.py: 4 warnings
tests/config/test_external_policy.py: 2 warnings
tests/config/test_orchestrator_config.py: 6 warnings
tests/core/test_imports.py: 7 warnings
tests/core/test_matching_engine_album_tracks.py: 2 warnings
tests/core/test_matching_engine_determinism.py: 1 warning
tests/core/test_matching_engine_edition_flag.py: 1 warning
tests/core/test_matching_engine_thresholds.py: 4 warnings
tests/core/test_transfers_api.py: 8 warnings
tests/core/test_types_contract.py: 6 warnings
tests/docs/test_workers_doc_exists.py: 1 warning
tests/e2e/test_artist_flow.py: 8 warnings
tests/integrations/test_artist_gateway.py: 3 warnings
tests/integrations/test_error_mapping.py: 6 warnings
tests/integrations/test_gateway_concurrency.py: 1 warning
tests/integrations/test_gateway_retries.py: 2 warnings
tests/integrations/test_health.py: 2 warnings
tests/integrations/test_normalizers.py: 6 warnings
tests/integrations/test_provider_gateway_artist_flow.py: 1 warning
tests/integrations/test_provider_gateway_new_endpoints.py: 2 warnings
tests/integrations/test_registry_enable_disable.py: 2 warnings
tests/integrations/test_slskd_adapter.py: 8 warnings
tests/integrations/test_soulseek_retry.py: 2 warnings
tests/logging/test_log_event.py: 4 warnings
tests/logging/test_router_events.py: 5 warnings
tests/middleware/test_cache_etag.py: 1 warning
tests/middleware/test_cache_middleware.py: 4 warnings
tests/middleware/test_pipeline.py: 6 warnings
tests/middleware/test_request_id.py: 1 warning
tests/middleware/test_security_profiles.py: 4 warnings
tests/migrations/test_artist_schema.py: 2 warnings
tests/migrations/test_queue_jobs_idempotency_index.py: 7 warnings
tests/migrations/test_upgrade_downgrade_sqlite.py: 4 warnings
tests/observability/test_logging_cache.py: 1 warning
tests/observability/test_logging_gateway.py: 2 warnings
tests/observability/test_logging_orchestrator.py: 1 warning
tests/observability/test_logging_workers.py: 1 warning
tests/orchestrator/test_artist_handlers.py: 5 warnings
tests/orchestrator/test_artist_sync.py: 6 warnings
tests/orchestrator/test_dispatcher.py: 11 warnings
tests/orchestrator/test_events.py: 1 warning
tests/orchestrator/test_metrics.py: 2 warnings
tests/orchestrator/test_retry_policy.py: 1 warning
tests/orchestrator/test_scheduler.py: 7 warnings
tests/orchestrator/test_sync_handler.py: 8 warnings
tests/orchestrator/test_timer.py: 6 warnings
tests/orchestrator/test_uses_central_config.py: 3 warnings
tests/routers/test_async_db_responsiveness.py: 10 warnings
tests/routers/test_defaults_flags.py: 7 warnings
tests/routers/test_download_errors.py: 12 warnings
tests/routers/test_legacy_router_shims.py: 7 warnings
tests/routers/test_request_deadlines.py: 4 warnings
tests/routers/test_search_logging.py: 2 warnings
tests/routers/test_spotify_free_import.py: 2 warnings
tests/routers/test_spotify_module.py: 2 warnings
tests/routers/test_spotify_routes_compat.py: 4 warnings
tests/schemas/test_common_schema_types.py: 18 warnings
tests/services/test_artist_dao.py: 8 warnings
tests/services/test_artist_dao_async.py: 4 warnings
tests/services/test_artist_delta.py: 9 warnings
tests/services/test_artist_workflow_dao.py: 6 warnings
tests/services/test_audit.py: 2 warnings
tests/services/test_cache_logging_and_ttl.py: 2 warnings
tests/services/test_download_service.py: 5 warnings
tests/services/test_errors_mapping.py: 7 warnings
tests/services/test_integration_service_gateway_delegation.py: 4 warnings
tests/services/test_library_service_dto_contract.py: 5 warnings
tests/services/test_retry_policy_provider.py: 5 warnings
tests/services/test_search_service_end_to_end_mocked.py: 1 warning
tests/services/test_spotify_domain_service.py: 6 warnings
tests/snapshots/test_openapi_schema.py: 1 warning
tests/spotify/test_playlist_cache_invalidation.py: 7 warnings
tests/test_activity.py: 6 warnings
tests/test_activity_export.py: 10 warnings
tests/test_activity_history.py: 12 warnings
tests/test_activity_persistence.py: 8 warnings
tests/test_api_auth.py: 11 warnings
tests/test_api_versioning.py: 4 warnings
tests/test_artists.py: 6 warnings
tests/test_artwork.py: 18 warnings
tests/test_backfill_router.py: 6 warnings
tests/test_backfill_service.py: 4 warnings
tests/test_cache_headers.py: 8 warnings
tests/test_cache_store.py: 3 warnings
tests/test_config.py: 4 warnings
tests/test_dlq_router.py: 18 warnings
tests/test_dlq_service.py: 10 warnings
tests/test_download.py: 34 warnings
tests/test_download_credentials.py: 2 warnings
tests/test_download_export.py: 12 warnings
tests/test_download_filters.py: 4 warnings
tests/test_download_flow_with_artwork.py: 2 warnings
tests/test_download_priority.py: 18 warnings
tests/test_download_retry.py: 13 warnings
tests/test_e2e_smoke.py: 4 warnings
tests/test_feature_flags.py: 9 warnings
tests/test_file_organization.py: 7 warnings
tests/test_free_ingest.py: 3 warnings
tests/test_free_ingest_router.py: 8 warnings
tests/test_health_endpoints.py: 12 warnings
tests/test_health_ready.py: 12 warnings
tests/test_imports_free.py: 20 warnings
tests/test_integrations.py: 3 warnings
tests/test_lifespan_workers.py: 5 warnings
tests/test_lyrics.py: 7 warnings
tests/test_matching.py: 5 warnings
tests/test_metadata.py: 17 warnings
tests/test_migrations_env.py: 2 warnings
tests/test_search.py: 17 warnings
tests/test_secret_validation.py: 17 warnings
tests/test_settings.py: 2 warnings
tests/test_settings_defaults.py: 8 warnings
tests/test_soulseek.py: 18 warnings
tests/test_spotify.py: 12 warnings
tests/test_spotify_free_mode.py: 14 warnings
tests/test_spotify_mode_gate.py: 4 warnings
tests/test_status_connections.py: 2 warnings
tests/test_sync.py: 8 warnings
tests/test_sync_credentials.py: 2 warnings
tests/test_system.py: 4 warnings
tests/test_text_normalization.py: 3 warnings
tests/test_watchlist.py: 4 warnings
tests/test_worker_activity.py: 8 warnings
tests/test_worker_health.py: 8 warnings
tests/test_workers.py: 2 warnings
tests/utils/test_artwork_utils.py: 1 warning
tests/utils/test_concurrency.py: 2 warnings
tests/utils/test_idempotency_utils.py: 4 warnings
tests/utils/test_jsonx_validate.py: 3 warnings
tests/utils/test_priority.py: 3 warnings
tests/utils/test_retry.py: 4 warnings
tests/utils/test_service_health.py: 2 warnings
tests/utils/test_spotify_free.py: 6 warnings
tests/utils/test_time.py: 3 warnings
tests/workers/test_artist_sync_handler.py: 10 warnings
tests/workers/test_async_session_yielding.py: 4 warnings
tests/workers/test_backfill_worker.py: 2 warnings
tests/workers/test_enqueue_concurrency.py: 8 warnings
tests/workers/test_graceful_shutdown.py: 1 warning
tests/workers/test_idempotency.py: 1 warning
tests/workers/test_matching_worker.py: 6 warnings
tests/workers/test_matching_worker_shutdown.py: 2 warnings
tests/workers/test_orch_pools.py: 2 warnings
tests/workers/test_orch_priority.py: 3 warnings
tests/workers/test_orch_retries_dlq.py: 1 warning
tests/workers/test_orch_visibility.py: 4 warnings
tests/workers/test_playlist_sync_worker.py: 2 warnings
tests/workers/test_retries_and_backoff.py: 1 warning
tests/workers/test_sync_worker.py: 4 warnings
tests/workers/test_sync_worker_priority.py: 4 warnings
tests/workers/test_sync_worker_restart.py: 2 warnings
tests/workers/test_visibility_timeout.py: 1 warning
tests/workers/test_watchlist_cooldown.py: 2 warnings
tests/workers/test_watchlist_defaults.py: 4 warnings
tests/workers/test_watchlist_timer.py: 2 warnings
tests/workers/test_watchlist_worker.py: 3 warnings
tests/workers/test_worker_config_logging.py: 4 warnings
  /root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/alembic/config.py:598: DeprecationWarning: No path_separator found in configuration; falling back to legacy splitting on spaces, commas, and colons for prepend_sys_path.  Consider adding path_separator=os to Alembic config.
    util.warn_deprecated(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=================================================== short test summary info ====================================================
FAILED tests/e2e/test_artist_flow.py::test_artist_flow_happy_path_persists_and_exposes_via_api - AssertionError: assert {'api...
FAILED tests/orchestrator/test_scheduler.py::test_scheduler_leases_jobs_in_priority_order - AssertionError: assert [] == [('m...
FAILED tests/routers/test_search_logging.py::test_search_router_emits_api_request_event - AssertionError: Expected api.reques...
FAILED tests/snapshots/test_openapi_schema.py::test_openapi_snapshot - AssertionError: assert {'components'...}}, ...}, ...} ...
FAILED tests/spotify/test_playlist_cache_invalidation.py::test_playlist_list_busts_cache_on_update_response - assert 304 == 200
FAILED tests/test_spotify.py::test_playlist_sync_worker_persists_playlists - AssertionError: assert 'Focus' == 'Focus Updated'
FAILED tests/test_spotify.py::test_playlist_cache_invalidation - AssertionError: assert 'Focus' == 'Morning Updated'
FAILED tests/test_spotify_mode_gate.py::test_spotify_pro_features_require_credentials - assert True is False
FAILED tests/test_system.py::test_system_stats_endpoint_uses_psutil - assert 13.7 == 30.0
FAILED tests/workers/test_matching_worker.py::test_handle_matching_invalid_payload - app.orchestrator.handlers.MatchingJobErr...
FAILED tests/workers/test_matching_worker.py::test_handle_matching_no_candidates_above_threshold - app.orchestrator.handlers....
FAILED tests/workers/test_watchlist_worker.py::test_watchlist_handler_retryable_failure_reschedules - AssertionError: assert ...
ERROR tests/api/test_admin_artists.py::test_admin_dry_run_shows_delta_no_side_effects - sqlalchemy.exc.OperationalError: (sql...
ERROR tests/api/test_admin_artists.py::test_admin_apply_reconcile_updates_and_audits - sqlalchemy.exc.OperationalError: (sqli...
ERROR tests/api/test_admin_artists.py::test_admin_resync_enqueues_with_priority_and_lock_guard - sqlalchemy.exc.OperationalEr...
ERROR tests/api/test_admin_artists.py::test_admin_audit_lists_recent_events_paginated - sqlalchemy.exc.OperationalError: (sql...
ERROR tests/api/test_admin_artists.py::test_admin_invalidate_busts_cache_etag - sqlalchemy.exc.OperationalError: (sqlite3.Ope...
ERROR tests/api/test_admin_artists.py::test_admin_safety_checks_retry_budget_and_staleness - sqlalchemy.exc.OperationalError:...
12 failed, 627 passed, 2 skipped, 943 warnings, 6 errors in 273.61s (0:04:33)
